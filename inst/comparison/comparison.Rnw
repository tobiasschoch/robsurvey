\documentclass[a4paper,oneside,11pt,DIV=12]{scrartcl}

\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\usepackage{times}
\usepackage{framed}
\usepackage{amsmath,amsfonts,amssymb,bm}
\usepackage{Sweave}
\usepackage{setspace}
\usepackage[longnamesfirst]{natbib}

\newcommand{\code}[1]{{\texttt{#1}}}

\setlength\parindent{24pt}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% ==============================================================================
\begin{document}
\shortcites{robustbase}

\title{Comparison with other implementations of
    regression $M$- and $GM$-estimators}

\author{{\normalsize Tobias Schoch} \\
\begin{minipage}[t][][t]{\textwidth}
	\begin{center}
	\small{University of Applied Sciences Northwestern Switzerland FHNW} \\
	\small{School of Business, Riggenbachstrasse 16, CH-4600 Olten} \\
	\small{\texttt{tobias.schoch{@}fhnw.ch}}
	\end{center}
\end{minipage}}

\date{{\small \today}}
\maketitle

\renewenvironment{abstract}{%
\begin{center}\begin{minipage}{0.9\textwidth}
\rule{\textwidth}{0.4pt}
{\sffamily\bfseries\footnotesize Abstract.}\small}
{\par\noindent\rule{\textwidth}{0.4pt}\end{minipage}\end{center}}

\begin{abstract}
In this report, we study the behavior of the methods \code{svyreg\_huber}
and \code{svyreg\_huberGM} in package \code{robsurvey} with other
implementations. We restricted attention to studying the methods for
4 well-known datasets. For all datasets under study, our implementations
are identical (in terms of floating point arithmetic) with results of
the competing implementations. Although our comparisons provide only
anecdotal evidence on the performance of the methods, we believe
that the comparisons shed some light on the behavior of our
implementations. We are fairly confident that the methods in package
\code{robsurvey} behave the way they are supposed to.
\end{abstract}

\vspace{1em}

\setstretch{1.1}

<<echo=false>>=
library(robsurvey, quietly = TRUE)
library(survey)
library(MASS)
library(robustbase)
library(robeth)
source("R_functions.R")
@

%-------------------------------------------------------------------------------
\section{Introduction}\label{sec:intro}
In this short report, we compare the behavior of the regression $M$- and
$GM$-estimators in package \code{robsurvey} with the methods from
other implementations. To this end, we study the estimated parameters
for 4 well-known datasets/ cases studies. With regard to competing
implementations, we consider the methods from the following \code{R}
packages:
<<echo=false>>=
pkg <- c("MASS", "robeth")
for (i in 1:length(pkg))
    cat(paste0(pkg[i], ", version: ", packageVersion(pkg[i]),"\n"))
@
\noindent These packages are documented in, respectively, \cite{mass}
and \cite{robeth}. The datasets are from package
<<echo=false>>=
pkg <- "robustbase"
cat(paste0(pkg, ", version: ", packageVersion(pkg),"\n"))
@
\noindent see \cite{robustbase}. In all comparisons, we
\begin{itemize}
    \item study $M$- or $GM$-estimators with the MAD (normalized median
        absolute deviation) as estimator of scale;
    \item use the robustness tuning constant \code{k = 1.345} of the
        Huber $\psi$-function;
    \item focus on sample data that do not contain sampling weights.
\end{itemize}
\noindent All studied methods compute the regression estimates by
iteratively reweighted least squares (IRWLS) and the estimate of scale
(more precisely, the trial value for the scale estimate) is updated at
each iteration.

\begin{leftbar}
\textbf{Limitations:} Our comparisons provide only anecdotal evidence
on the performance of the methods. Nonetheless, we believe that the
comparisons shed some light on the behavior of our implementations.
\end{leftbar}

Let $\bm x$ and $\bm y$ denote two real-valued $p$-vectors. We define
the absolute relative difference by
\begin{equation*}
    \mathrm{absrelDIFF}(\bm x, \bm y) = 100\% \cdot \max_{i=1, \ldots, p}
    \left\{ \Big\vert \frac{x_i}{y_i} - 1 \Big\vert \right\}.
\end{equation*}

The remainder of the paper is organized as follows. In Section
\ref{sec:mest}, we compare several implementations of the Huber
$M$-estimator of regression. Section \ref{sec:gmest} studies
implementations of the Huber $GM$-estimator of regression. In
Section \ref{sec:summary}, we summarize the findings.

%-------------------------------------------------------------------------------
\section{Huber $M$-estimators of regression}\label{sec:mest}
In this section, we study the Huber $M$-estimator of regression. The
parametrizations of the algorithms have been chosen to make them
comparable; we use:
\begin{itemize}
    \item \code{MASS::rlm}: \code{method = "M"},
        \code{scale.est = "MAD"}, \code{acc = 1e-5},
        \code{test.vec = "coef"}, and \code{maxit = 50},
    \item \code{robeth::rywalg}: \code{tol = 1e-5}, \code{maxit = 50},
        \code{itype = 1}, \code{isigma = 2}, \code{icnv = 1},
        and \code{maxis = 1}; see \cite{marazzi1993} for more details.
    \item \code{robsurvey::svyreg\_huber}: \code{tol = 1e-5},
        and \code{maxit = 50}.
\end{itemize}

\noindent The methods \code{MASS::rlm} and \code{robeth::rywalg} compute
the regression scale estimate by the (normalized) median of the absolute
deviations (MAD) \emph{about zero}. The method \code{robsurvey::svyreg\_huber}
(and \code{svyreg\_tukey}) implements two variants of the MAD:
\begin{itemize}
    \item \code{mad\_center = FALSE}: MAD centered about zero,
    \item \code{mad\_center = TRUE}: MAD centered about the (weighted) median.
    (This is the default).
\end{itemize}

\noindent For ease of reference, we denote the MAD centered about zero by
\code{mad0}.

In practice, the estimate of regression and scale differ
whether the MAD is centered about zero or the median because the median
of the residuals is not exactly zero for empirical data. If the residuals
have a skewed distribution, the two variants of the MAD can differ by a lot.

\subsection{Case 1: education data}\label{sec:education}
The \code{education} data are on public education expenditures (at the
level of US states), and are from \cite{chatterjeeprice}
[see \cite{chatterjeehadi} for a newer edition]; see also
\cite{rousseeuwleroy}. The dataset contains 4 variables: the
response variable (\code{Y}: per capita expenditure on public
education in a state, projected for 1975) and the three explanatory
variables
\begin{itemize}
    \item \code{X1}: Number of residents per thousand residing in urban
        areas in 1970,
    \item \code{X2}: Per capita personal income in 1973,
    \item \code{X3}: Number of residents per thousand under 18 years of
        age in 1974.
\end{itemize}

\noindent There are 50 observations on the 4 variables. The first 6
    rows of the data are shown below.
<<>>==
data(education, package = "robustbase")
head(education)
@

\noindent The following tabular output shows the estimated coefficients
(and the estimated scale; last column) under the model
\code{Y $\sim$ X1 + X2 + X3} for 4 different implementations/ methods.
<<echo=FALSE>>=
M_compare(Y ~ X1 + X2 + X3, education)
@

\noindent The estimates of the 4 methods differ only slightly. We have the
following findings:
\begin{itemize}
    \item \code{svyreg\_huber (mad0)} is based on the MAD centered about
        zero. In methodological terms, it is identical with the
        implementations \code{rlm (MASS)} and \code{rywalg (ROBETH)}. The
        estimates of \code{svyreg\_huber (mad0)} are virtually identical
        with the ones of \code{rlm (MASS)}.  The estimates of
        \code{rywalg (ROBETH)} deviate more from the other methods.
    \item \code{svyreg\_huber} is based on the MAD centered about the
        (weighted) median. The estimates differ slightly from
         \code{svyreg\_huber (mad0)}.
\end{itemize}

\noindent The discrepancies are mainly due to the normalization constant
to make the MAD an unbiased estimator of the scale at the Gaussian core
model. In \code{rlm (MASS)}, the MAD about zero is computed by
\code{median(abs(resid)) / 0.6745}. The constant $1 / 0.6745$ is
equal to $1.482580$ (with a precision of 6 decimal places), which
differs slightly from $1/\Phi^{-1}(0.75)=1.482602$, where $\Phi$ denotes
the cumulative distribution function of the standard Gaussian.
The implementation of \code{svyreg\_huber} uses $1.482602$
(see file \code{src/constants.h}). Now, if we replace $1 / 0.6745$ in the
above code snippet by $1.482602$ in the function body of \code{rlm.default},
then the regression coefficients of the so modified code and
\code{svyreg\_huber} are (in terms of floating point arithmetic) almost
identical. The absolute relative difference is

<<echo=FALSE>>=
design <- svydesign(id = ~1, weights = rep(1, nrow(education)),
    data = education)
m1 <- svyreg_huber(Y ~ X1 + X2 + X3, design, k = 1.345,
        mad_center = FALSE, tol = 1e-5, maxit = 50)

rlm_mod <- MASS:::rlm.default
body(rlm_mod)[[22]][[4]][[3]][[3]][[2]][[3]][[3]][[3]] <-
    substitute(median(abs(resid)) * 1.482602)

m2 <- rlm_mod(m1$model$x, m1$model$y, k = 1.345, method = "M",
     scale.est = "MAD", acc = 1e-5, maxit = 50, test.vec = "coef")

cat("\nabsrelDIFF: ", 100 * max(abs(coef(m1) / coef(m2) - 1)),
    "%\n")
@

Next, we consider comparing the estimated (asymptotic) covariance matrix of
the estimated regression coefficients. To this end, we computed the diagonal
elements of the estimated covariance matrix for the
methods \code{svyreg\_huber (mad0)} and \code{rlm (MASS)}; see below. In
addition, we computed the absolute relative difference between the two
methods.

<<echo=FALSE>>=
M_compare_cov(Y ~ X1 + X2 + X3, education)
@

\noindent The diagonal elements of the estimated covariance matrix differ
only slightly between the two methods. The discrepancies can be explained
by the differences in terms of the estimated coefficients.

%-------------------------------------------------------------------------------
\subsection{Case 2: stackloss data}\label{sec:stackloss}
The \code{stackloss} data consist of 21 measurements on the oxidation
of ammonia to nitric acid for an industrial process; see \cite{brownlee}.
The variables are:
\begin{itemize}
    \item \code{Air Flow}: flow of cooling air,
    \item \code{Water Temp}: cooling water inlet temperature,
    \item \code{Acid Conc.}: concentration of acid [per 1000, minus 500],
    \item \code{stack.loss}: stack loss.
\end{itemize}

\noindent The first 6 observations of the data are shown below.

<<>>==
data(stackloss, package = "datasets")
head(stackloss)
@

\noindent The variable \code{stack.loss} (stack loss of amonia) is
regressed on the explanatory variables air flow, water temperature and
the concentration of acid. The regression coefficients and the estimate
of scale are tabulated for the 4 implementations/ methods under study.

<<echo=FALSE>>=
M_compare(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., stackloss)
@

\noindent The estimates of the regression $M$-estimator which is based on
the MAD centered about zero are virtually identical (see rows 2--4). The
estimates of \code{svyreg\_huber} deviate slightly from the latter
because it is based on the MAD centered about the (weighted) median.

We did not repeat the analysis on differences in the estimated
covariance matrices because the results are qualitatively the same
as in Case 1.

% %-------------------------------------------------------------------------------
\section{Huber $GM$-estimators of regression}\label{sec:gmest}
In this section, we consider regression $GM$-estimators with Huber
$\psi$-function (tuning constant fixed at $k=1.345$). The scale is
estimated by MAD. With regard to the MAD, we distinguish two cases:
\code{svyreg\_huberGM} and \code{svyreg\_huberGM (mad0)}, where
\code{mad0} refers to the MAD about zero.

We computed the weights to downweight leverage observations
(\code{xwgt}) with the help of the methods in package \code{robeth}.
The so computed weights were then stored to be utilized in all
implementations of $GM$-estimators of regression. This approach ensures
that the implementations do not differ in terms of the \code{xgwt}'s.

%-------------------------------------------------------------------------------
\subsection{Case 3: delivery data}
The \code{delivery} data consist of observations on servicing 25
soft drink vending machines. The data are from \cite{montgomerypeck};
see also \cite{rousseeuwleroy}. The variables are:

\begin{itemize}
    \item \code{n.prod}: number of products stocked in the vending machine,
    \item \code{distance}: distance walked by the route driver (ft),
    \item \code{delTime}: delivery time (minutes).
\end{itemize}

\noindent The goal is to model/ predict the amount of time required by
the route driver to service the vending machines.  The variable
\code{delTime} is regressed on the variables
\code{n.prod} and \code{distance}. The first six observations
of the data are shown below.

<<>>==
data(delivery, package = "robustbase")
head(delivery)
@

\subsubsection{Mallows $GM$-estimator}\label{sec:delivery_mallows}
\noindent The regression coefficients and the estimate of scale are
tabulated for the 3 implementations/ methods under study.

<<echo=FALSE>>==
GM_mallows_compare(delTime ~ n.prod + distance, delivery)
@

\noindent The estimates of \code{svyreg\_huberGM (Mallows, mad0)} are
almost identical with results of \code{rywalg (ROBETH, Mallows)}; see
rows 2 and 3. The estimates of \code{svyreg\_huberGM (Mallows)} (i.e.,
based on the MAD centered about the weighted median differ slightly
as is to be expected.

\subsubsection{Schweppe $GM$-estimator}\label{sec:delivery_schweppe}

<<echo=FALSE>>==
GM_schweppe_compare(delTime ~ n.prod + distance, delivery)
@

\noindent The estimates of \code{svyreg\_huberGM (Schweppe, mad0)} and
\code{rywalg (ROBETH, Schweppe)} (see rows 2 and 3) are slightly
different. We could not figure out the reasons for this discrepancy.


%-------------------------------------------------------------------------------
\subsection{Case 4: salinity data}
The \code{salinity} data are a set of measurements of water salinity
and river discharge taken in North Carolina's Pamlico Sound;
\cite{ruppertcarroll}; see also \cite{rousseeuwleroy}. The variables are

\begin{itemize}
    \item \code{Y}: salinity,
    \item \code{X1}: salinity lagged two weeks,
    \item \code{X2}: linear time trend,
    \item \code{X3}: river discharge.
\end{itemize}

\noindent There a 28 observations. The first six observations of the data
are shown below.

<<>>==
data(salinity, package = "robustbase")
head(salinity)
@

\noindent We consider fitting the model \code{Y $\sim$ X1 + X2 + X3}
by several implementations of the regression $GM$-estimators.

\subsubsection{Mallows $GM$-estimator}
<<echo=FALSE>>==
GM_mallows_compare(Y ~ X1 + X2 + X3, salinity)
@

\noindent The differences between the estimates of
\code{svyreg\_huberGM (Mallows, mad0)} and \code{rywalg (ROBETH, Mallows)}
are larger (see rows 2 and 3) than in Case 3. Still, the estimates are
very similar.

\subsubsection{Schweppe $GM$-estimator}

<<echo=FALSE>>==
GM_schweppe_compare(Y ~ X1 + X2 + X3, salinity)
@

\noindent The estimates of \code{svyreg\_huberGM (Schweppe, mad0)} and
\code{rywalg (ROBETH, Schweppe)} (see rows 2 and 3) are slightly
different. But the differences are minor.


%-------------------------------------------------------------------------------
\section{Summary and conclusions}\label{sec:summary}
In this paper, we studied the behavior of the methods \code{svyreg\_huber}
and \code{svyreg\_huberGM} in package \code{robsurvey} with other
implementations. We restricted attention to studying the methods for
4 well-known datasets. For all datasets under study, our implementations
replicate (or are at least very close to) the results of the competing
implementations. Although our comparisons provide only anecdotal evidence
on the performance of the methods, we believe that the comparisons shed
some light on the behavior of our implementations.

%-------------------------------------------------------------------------------

\begin{thebibliography}{14}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[\protect\citeauthoryear{Brownlee}{Brownlee}{1965}]{brownlee}
    \textsc{Brownlee, K. A.} (1965).
    \emph{Statistical Theory and Methodology in Science and Engineering},
    2nd ed., John Wiley and Sons, New York.

\bibitem[\protect\citeauthoryear{Chatterjee and Price}{Chatterjee and Price}
        {1977}]{chatterjeeprice}
    \textsc{Chatterjee, S. and B. Price} (1977).
    \emph{Regression Analysis by Example},
    John Wiley and Sons, New York.

\bibitem[\protect\citeauthoryear{Chatterjee and Hadi}{Chatterjee and Hadi}
        {2012}]{chatterjeehadi}
    \textsc{Chatterjee, S. and A. S. Hadi} (2012).
    \emph{Regression Analysis by Example},
    5th ed., John Wiley and Sons, Hoboken (NJ).

\bibitem[\protect\citeauthoryear{M{\"a}chler, Rousseeuw, Croux, Todorov,
        Ruckstuhl, Salibian-Barrera, Verbeke, Koller, Conceicao, and di Palma}
        {M{\"a}chler et al.}{2020}]{robustbase}
    \textsc{M{\"a}chler, M., P. Rousseeuw, C. Croux, V. Todorov, A. Ruckstuhl,
    M.  Salibian-Barrera, T. Verbeke, M. Koller, E. L. T. Conceicao, and
    M. A. di Palma} (2019).
    \emph{robustbase: Basic Robust Statistics}.
    R package version 0.93-4.
    URL http://CRAN.R-project.org/package=robustbase

\bibitem[\protect\citeauthoryear{Marazzi}{Marazzi}{2020}]{robeth}
    \textsc{Marazzi, A. (2020).}
    \emph{robeth: R Functions for Robust Statistics}.
    R package version 2.7-6.
    URL https://CRAN.R-project.org/package=robeth

\bibitem[\protect\citeauthoryear{Marazzi}{Marazzi}{1993}]{marazzi1993}
    \textsc{Marazzi, A.} (1993).
    \emph{Algorithms, Routines, and S-Functions for Robust Statistics:
    The Fortran Library ROBETH with an interface to S-PLUS},
    Chapman and Hall/ CRC, New York. (with the collaboration of Johann
    Joss and Alex Randriamiharisoan)

\bibitem[\protect\citeauthoryear{Montgomery and Peck}{Montgomery and Peck}
        {2006}]{montgomerypeck}
    \textsc{Montgomery, D. C. and E. A. Peck} (2006).
    \emph{Introduction to Linear Regression Analysis},
    4th ed., John Wiley and Sons, Hoboken (NJ).

\bibitem[\protect\citeauthoryear{Rousseeuw and Leroy}{Rousseeuw and Leroy}
        {1987}]{rousseeuwleroy}
\textsc{Rousseeuw, P. J. and A. M. Leroy} (1987).
    \emph{Robust Regression and Outlier Detection},
    John Wiley and Sons, Hoboken (NJ).

\bibitem[\protect\citeauthoryear{Ruppert and Carroll}{Ruppert and Carroll}
        {1980}]{ruppertcarroll}
    \textsc{Ruppert, D. and R. J. Carroll} (1980).
    Trimmed Least Squares Estimation in the Linear Model.
    \emph{Journal of the American Statistical Association} 75,
    pp. 828--838.

\bibitem[\protect\citeauthoryear{Venables and Ripley}{Venables and Ripley}
        {2002}]{mass}
    \textsc{Venables, W. N. and B.D. Ripley} (2002).
    \emph{Modern Applied Statistics with S},
    4th ed., Springer, New York.
\end{thebibliography}

%-------------------------------------------------------------------------------
\appendix

\section{R session information}
<<echo=false,results=tex>>=
toLatex(sessionInfo(), locale = FALSE)
@

\end{document}
