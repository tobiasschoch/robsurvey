\documentclass[a4paper,11pt]{scrreprt}
\setlength{\textheight}{1.1\textheight}

\usepackage{amssymb,amsmath,amsthm,mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[longnamesfirst]{natbib}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{enumerate}

\usepackage[T1]{fontenc}
\usepackage{times}

\usepackage{setspace}
\onehalfspacing
\setlength\parindent{24pt}

\include{sctmath}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\code}[1]{{\texttt{#1}}}


%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

\title{\Large RobSurvey}
\author{Beat Hulliger, Tobias Schoch, and Martin Sterchi}
\date{{\small \today}}
\maketitle
\tableofcontents

%------------------------------------------------------------------------------
\chapter{Introduction}\label{ch:introduction}


%------------------------------------------------------------------------------
\chapter{Weighted wuantile, weighted trimmed and winsorized means}\label{ch:basic}


%------------------------------------------------------------------------------
\section{Quantile}
Consider the study variable $y_i$, $i \in U$. The population distribution function of $y_i$ is denoted by $F_{y,N}$ or just $F_N$ for notational convenience (i.e., the dependence on $y$ is suppressed), 
\begin{equation*}
   F_N(y) = \frac{1}{N}\sum_{i \in U} \indicator\{y_i \leq y \}, 
\end{equation*}
\noindent where $\indicator\{ A \}$ is the indicator function which is equal to one if condition $A$ holds and zero otherwise. A sample-based estimate is given by 
\begin{equation*}
   \widehat{F}(y) = \frac{1}{\widehat{N}}\sum_{i \in s} w_i \indicator\{y \leq y_i \} \qquad \text{with} \qquad \widehat{N} = \sum_{i \in s} w_i,  
\end{equation*}
\noindent where $w_i$ is the sampling weight. Although estimator $\widehat{F}$ may be biased as an estimator of $F_N$, it is consistent \citep[][]{sarndaletal1992}. The function \texttt{svycdf} in package \texttt{survey} can be used to compute $\widehat{F}$, and the corresponding plot method displays the estimated distribution function; see \citet[][ch. 4.3.1]{lumley2010} for more details. 
  
Let $Q_y(p) = F_{y,N}^{-1}(p)$ be the population quantile function. We use the shorthand notation $Q_p$ for the $p$-th population quantile $Q_y(p)$, $p \in [0,1]$, whenever no confusion can arise. Formally, it is defined as (for $0 \leq p \leq 1$)
\begin{equation*}
   Q_p = \inf \{ y : F_N(y) \geq p \}.
\end{equation*}
\noindent A sample-based estimate of the population quantile $Q_p$ is denoted by $\widehat{Q}_p$. The estimation of quantiles poses some technical difficulties because they depend less smoothly on the data than most calculated population characteristics (e.g., means or totals). Consider the median for a sample of even size. Even under simple random sampling, the median is not uniquely defined. Hence, any number between the two central observations (of the sorted data) is a valid choice. Most definitions of the sample quantile interpolate linearly between two adjacent observations when the quantile is not uniquely defined. The method \texttt{survey::svyquantile} is quite general in this regard because it allows the user to specify the interpolation method. Moreover, it can be used to compute standard error and confidence intervals for $\widehat{Q}_p$; see \citet[][ch. 2.4.1]{lumley2010}. Our definition of $\widehat{Q}_p$ obtains by setting arguments the \texttt{method = "constant"} and \texttt{f=0} in \texttt{survey::svyquantile}. Another difficulty with the estimation of quantiles arises in the presence of tied or discrete data. With tied observations, we have to decide whether two (or more) observations with the same value are handled as one unit (with summed weights) or not. The discussion of tied observations is rather technical and we refer to reader to \citet[][ch. 2.4.1]{lumley2010} and the references therein.

%------------------------------------------------------------------------------
\subsection{Implementation}
Let $y_{(1)}, \ldots, y_{(n)}$ denote the $n$ (sample) order statistics of the $y$-variable (i.e., the $y_i$'s sorted in ascending order). Moreover, we sort the weights $w_i$ along the order of the sorted $y_i$'s; the so sorted weights are denoted by $w_{(i)}$ ($i=1,\ldots,n$). Next, we determine the largest possible integer $I$ (with $I=1,2,\ldots$) such that the sum of the sorted weights, $w_{(1)} + \ldots + w_{(I)}$, satisfies the inequality, 
\begin{equation*}
   \sum_{i=1}^{I} w_{(i)} \leq \alpha \times \hat{t}_w, \qquad \text{where} \quad \hat{t}_w = \sum_{i \in S} w_i. 
\end{equation*}
\noindent Having determined $I$, the estimated $\alpha$-quantile $\hat{y}_{(\alpha)}$ is now defined as the $I$-th order statistic $y_{(I)}$.

C.A.R. Hoare's algorithms \texttt{quickselect} and \texttt{quicksort} \citep[see e.g.][p. 113--123, and Exercise 31 on p. 136]{knuth1998v3}


Let $i$ denote the pivot's position. We use C style zero indexing. Define (for any $0\leq k \leq n-1$)  
\begin{equation*}
   S_k = \sum_{j=0}^k \tilde{w}_j,
\end{equation*}
\noindent where 
\begin{equation*}
   \tilde{w}_j = \frac{w_j}{\sum_{k=0}^{n-1}w_k},
\end{equation*}
\noindent is the normalized weight (i.e., $\tilde{w}_0 + \ldots + \tilde{w}_{n-1} = 1$). With this, we have
\begin{equation*}
   S_{n-1} - S_i = \sum_{j=i+1}^{n-1}\tilde{w}_j,
\end{equation*}
\noindent which is the sum of the weights above the pivotal element $i$. For the $q$th weighted quantile, the active partition in Hoare's partitioning scheme is chosen as 
\begin{align*}
   \mathrm{right} &\leftarrow i - 1 \qquad \text{if} \quad  S_{i-1} \geq q, \\
   %
   \mathrm{left} &\leftarrow i + 1  \qquad \text{if} \quad  S_{n-1} - S_{i} > 1-q,  
\end{align*}
\noindent where $i$ denotes the pivot's position. If 
\begin{equation*}
   S_{i-1} < q \quad \text{and} \quad  S_{n-1} - S_{i} \leq 1-q  
\end{equation*}
\noindent the $q$th weighted quantile is obtained. 

 

%------------------------------------------------------------------------------
\section{Weighted trimmed and winsorized mean}
For notational convenience, we write $Q_p$ to mean the $p$-th population quantile and $\widehat{Q}_p$ for its estimate. The sample estimate of the symmetrically $\alpha$-trimmed population $y$-mean, where $\alpha$ is a given proportion $\alpha \in [0,0.5)$, is 
\begin{equation*}
   \widehat{m}_y^{\alpha, trim} =  \frac{\sum_{i \in S} w_i y_i  \indicator_i }{\sum_{i \in S} w_i \indicator_i} \qquad \text{with} \qquad  \indicator_i = \indicator \big\{\widehat{Q}_{\alpha} \leq y_i \leq \widehat{Q}_{1-\alpha} \big\},
\end{equation*}
\noindent  where $\indicator\{\cdot\}$ is the indicator function. If desired, we can obtain an asymmtrically trimmed mean by replacing $\widehat{Q}_{1-\alpha}$ in the above formula with, say, $\widehat{Q}_{\beta}$, where $\beta \in (\alpha, 1]$. 

Estimator $\widehat{m}_y^{\alpha, trim}$ discards all observations that are either smaller than the estimated $\alpha$-quantile or larger than the estimated $(1-\alpha)$-quantile. 

winsorized mean

\begin{equation*}
   \tilde{y}_i = \begin{cases} 
      \widehat{Q}_{\alpha} & \text{if} \quad y_i < \widehat{Q}_{\alpha}, \\
      %
      y_i & \text{if} \quad \widehat{Q}_{\alpha} \leq y_i \leq \widehat{Q}_{1-\alpha}, \\
      %
      \widehat{Q}_{1-\alpha} & \text{if} \quad y_i > \widehat{Q}_{1-\alpha}
   \end{cases}
\end{equation*}
\noindent is defined 
\begin{equation}\label{eq:winsorizedmeanwgt}
   \widehat{m}_y^{\alpha, wins} = \frac{\sum_{i \in S}w_i \tilde{y}_i}{\sum_{i \in S} w_i} 
\end{equation}

 let $y_{(1:n)}, y_{(2:n)}, \ldots, y_{(n:n)}$ denote the $n$ order statistics of the $y$-variable (i.e., the $y_i$'s sorted in ascending order). If the largest $k$ observations are regarded as outliers, we replaced them by the $k$-th largest value $y_{(k:n)}$. Formally, we define the winsorized $y$-variable, 
\begin{equation*}
   \tilde{y}_i = \begin{cases} 
      y_i & \text{if} \quad  y_i \leq y_{(k:n)}, \\
      %
      y_{(k:n)} & \text{if} \quad y_i > y_{(k:n)},
   \end{cases}
\end{equation*}
\noindent and compute (\ref{eq:winsorizedmeanwgt}) to get the one-sided $k$-winsorized mean.


instead of altering the $y$-values, an estimator can reduce the weights 

%------------------------------------------------------------------------------
\section{Box: Trimmed and winsorized mean}
Consider $n$ independent and indentically real-valued r.v.'s $Y_i$ ($i=1,\ldots,n$) with cdf $F$. Let $\alpha \in [0, 0.5)$ be a fixed proportion. The $\alpha$-trimmed sample $y$-mean, $\widehat{T}$, and the statistical functional corresponding to the trimmed mean, $T(F)$, are defined by, respectively, 
\begin{equation*}
   \widehat{T} = \frac{1}{n-2\lfloor \alpha n\rfloor}\sum_{i=\lfloor \alpha n\rfloor + 1}^{n - \lfloor \alpha n\rfloor}y_{(i:n)} \qquad  \text{and} \qquad  T(F) = \frac{1}{1 - 2\alpha} \int_{\alpha}^{1-\alpha} u \mathrm{d} F^{-1}(u), 
\end{equation*}
\noindent where $y_{(i:n)}$ is the $i$th order statistic, $F^{-1}$ is the quantile function, and $\lfloor u \rfloor$ denotes the greatest integer less than or equal to $u$. Under very general conditions on $F$ (see Stigler, 1973), the large sample distribution of $\sqrt{n}(\widehat{T} - T)$ is normal with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(T,y_i)/n$, where $\mathrm{IF}$ is the influence function of $T$ and is given by (for any $z \in \R$) 
\begin{align*}
   \mathrm{IF}\big(T, z\big) = \frac{1}{1-2\alpha}
   \begin{cases}
      F^{-1}(\alpha) - W(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - W(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      % 
      F^{-1}(1-\alpha) - W(F) & \text{if} \quad z > F^{-1}(1-\alpha), 
   \end{cases}
\end{align*}
\noindent where
\begin{equation}\label{eq:functionalwinsorized}
   W(F) = (1 - 2\alpha )T(F) + \alpha F^{-1}(\alpha) + \alpha F^{-1}(1-\alpha)
\end{equation}
\noindent is the functional corresponding to the $\alpha$-winsorized mean; see e.g. \citet[][p. 58]{huber1981}. An estimate of $W$, say $\widehat{W}$, obtains by replacing the quantiles $F^{-1}(\alpha)$ and $F^{-1}(1-\alpha)$ in (\ref{eq:functionalwinsorized}) with sample-based quantiles and substituting $\widehat{T}$ for $T$. Estimator $\widehat{W}$ has (when properly scaled) a limiting normal distribution with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(W,y_i) / n$, where \citep[see e.g.][58--59]{huber1981}
\begin{align*}
   \mathrm{IF}\big(W, z\big) = 
   \begin{cases}
      F^{-1}(\alpha) - \frac{\alpha}{f\big(F^{-1}(\alpha)\big)} - C(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - C(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(1-\alpha),\\
      % 
      F^{-1}(1-\alpha) + \frac{\alpha}{f\big(F^{-1}(1-\alpha)\big)}- C(F) & \text{if} \quad z > F^{-1}(1-\alpha), 
   \end{cases}
\end{align*}
\noindent where functional $C$ is defined as
\begin{equation*}
   C(F) = W(F) - \frac{\alpha^2}{f\big(F^{-1}(\alpha)\big)} - \frac{\alpha^2}{f\big(F^{-1}(1-\alpha)\big)}. 
\end{equation*}
\noindent Observe that the influence function has jumps at $F^{-1}(\alpha)$ and $F^{-1}(1-\alpha)$ and that it depends on the probability density function $f$. 




%------------------------------------------------------------------------------
\section{Variances}
r.v. $Y$ defined on $\R$ with cdf $F$. The trimmed mean is defined as $(\alpha,\beta)$ where $0 \leq \alpha < 1- \beta \leq 1)$
\begin{equation*}
   \widehat{T} = \frac{1}{n - \lfloor \alpha n\rfloor - \lfloor \beta n\rfloor}\sum_{i= \lfloor \alpha n \rfloor + 1}^{n - \lfloor \beta n \rfloor} y_i,
\end{equation*}
\noindent where $\lfloor u \rfloor$ denotes the greatest integer less than or equal to $u$. The statistical functional corresponding to the trimmed mean at the cdf $F$ is
\begin{equation*}
   T(F) = \frac{1}{1 - \alpha - \beta} \int_{\alpha}^{1-\beta} u \mathrm{d} F^{-1}(u), 
\end{equation*}
\noindent where $F^{-1}$ is the inverse of distribution function $F$. Under very general conditions on $F$ (see Stigler, 1973), the large sample distribution of $\sqrt{n}(\widehat{T} - T)$ is normal with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(T,y_i)/n$, where $\mathrm{IF}$ is the influence function of $T$ and is given by (for any $z \in \R$) 
\begin{align*}
   \mathrm{IF}\big(T, z\big) = \frac{1}{1-\alpha - \beta}
   \begin{cases}
      F^{-1}(\alpha) - W(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - W(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      % 
      F^{-1}(1-\beta) - W(F) & \text{if} \quad z > F^{-1}(1-\beta), 
   \end{cases}
\end{align*}
\noindent where
\begin{equation}\label{eq:functionalwinsorized}
   W(F) = (1 - \alpha -\beta)T + \alpha F^{-1}(\alpha) + \beta F^{-1}(1-\beta)
\end{equation}
\noindent is the functional corresponding to the $(\alpha,\beta)$-winsorized mean; see e.g. \citet[][p. 58]{huber1981}. An estimate of $W$, say $\widehat{W}$, obtains by replacing the quantiles $F^{-1}(\alpha)$ and $F^{-1}(1-\beta)$ in (\ref{eq:functionalwinsorized}) with sample-based quantiles and substituting $\widehat{T}$ for $T$. Estimator $\widehat{W}$ has (when properly scaled) a limiting normal distribution with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(W,y_i) / n$, where \citep[see e.g.][58--59]{huber1981}
\begin{align*}
   \mathrm{IF}\big(W, z\big) = 
   \begin{cases}
      F^{-1}(\alpha) - \frac{\alpha}{f\big(F^{-1}(\alpha)\big)} - C(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - C(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      % 
      F^{-1}(1-\beta) + \frac{\beta}{f\big(F^{-1}(1-\beta)\big)}- C(F) & \text{if} \quad z > F^{-1}(1-\beta), 
   \end{cases}
\end{align*}
\noindent where functional $C$ is defined as
\begin{equation*}
   C(F) = W(F) - \frac{\alpha^2}{f\big(F^{-1}(\alpha)\big)} - \frac{\beta^2}{f\big(F^{-1}(1-\beta)\big)}. 
\end{equation*}
\noindent Observe that the influence function has jumps at $F^{-1}(\alpha)$ and $F^{-1}(1-\beta)$ and that it depends on the probability density function $f$. 

%------------------------------------------------------------------------------
\chapter{Robust Horvitz--Thompson estimator}\label{ch:ht}
Define the initial estimates of location and scale to be
\begin{equation*}
   \mu^{\{0\}} = \widehat{\mathrm{med}}_{x,w} \qquad \text{and} \qquad s^{\{0\}} = \widehat{\mathrm{IQR}}_{x,w}.
\end{equation*}
\noindent For all iterations $\tau=0,1,2, \ldots$, define the winsorized $y$-variable by
\begin{equation*}
   \tilde{y}_i^{\{\tau\}} = \min\big\{ \max\big\{ \mu^{\{\tau\}} - k \cdot s^{\{\tau\}}, \; y_i \big\},\; \mu^{\{\tau\}} + k \cdot s^{\{\tau\}} \big\}, \qquad (i=1,\ldots,n)
\end{equation*}
\noindent where the superscript $\{\tau\}$ denotes the $\tau$-th iteration. Compute the updated location, 
\begin{equation*}
   \mu^{\{\tau + 1\}} = \frac{\sum_{i=1}^n w_i \tilde{y}_i^{\{\tau\}} }{\sum_{i=1}^n w_i},
\end{equation*}
\noindent and the updated scale,
\begin{equation*}
   s^{\{\tau + 1\}} = \sqrt{v^{\{\tau + 1\}}/\kappa},
\end{equation*}
\noindent where  $\kappa$ is a correction term to achieve Fisher consistency of the scale estimate at the Gaussian model, and
\begin{equation*}
   v^{\{\tau + 1\}} = \frac{\sum_{i=1}^n w_i \big(\tilde{y}^{\{\tau\}} - \mu^{\{\tau\}}\big)^2}{\sum_{i=1}^n w_i}.
\end{equation*}
\noindent The iterative updating scheme is stopped when the termination rule
\begin{equation*}
   \Big\vert \mu^{\{\tau + 1\}} - \mu^{\{\tau\}} \Big\vert  < \mathrm{tol} \cdot s^{\{\tau\}} \qquad \text{and} \qquad \left\vert \frac{s^{\{\tau + 1\}}}{s^{\{\tau\}}} - 1 \right\vert < \mathrm{tol}
\end{equation*}
\noindent is satisfied, where $\mathrm{tol}$ is a small number (e.g. $10^{-6}$). 

Let $\hat{\mu}$ and $\hat{s}$ denote the (final) estimates of $\mu$ and $s$. For the observations $i=1,\ldots,n$, let  
\begin{equation*}
   r_i = \frac{y_i - \hat{\mu}}{\hat{s}}
\end{equation*}
\noindent denote the residuals (\texttt{resid}) and define the robustness weight (\texttt{robwgt}) by $\psi_k(r_i)/r_i$. 


%------------------------------------------------------------------------------
\subsection{Box M-estimator}
Consider the model $Y_i = \theta + E_i$, where the random variables $E_i$ are independent with $\mathbbm{E}(E_i)=0$ and $\mathbbm{E}(E_i^2)=\sigma^2$ for all $i=1,\ldots,n$. The parameters $\theta \in \R$ and $\sigma^2 > 0$ are to be estimated. We denote the cdf of the $E_i$'s by $F$ and assume that it has a probability density function $f$ from the family of location-scale models, 
\begin{equation*}
   \frac{1}{\sigma}f\left( \frac{y_i - \theta}{\sigma}\right).
\end{equation*}
\noindent The family of location-scale models includes the Gaussian distribution and also symmetric distributions with heavier tails than the Gaussian (e.g., Laplace- and Student $t$-distribution). Under our parametric model, the joint maximum likelihood estimator, $(\hat{\theta}, \hat{\sigma})$, of the parameter vector $(\theta, \sigma)$ maximizes the likelihood function 
\begin{equation*}
   \prod_{i=1}^n  \frac{1}{\sigma}f\left( \frac{y_i - \theta}{\sigma}\right).
\end{equation*}
\noindent Equivalently, $(\hat{\theta}, \hat{\sigma})$ can be expressed as the solution to the system of score equations
\begin{align}
   \sum_{i=1}^n \psi\left( \frac{y_i - \theta}{\sigma}\right) &= 0 \\
   %
   \sum_{i=1}^n \psi\left( \frac{y_i - \theta}{\sigma}\right) \frac{y_i - \theta}{\sigma} &= 0, 
\end{align}
\noindent where
\begin{equation*}
   \psi(u) = -\frac{d}{d u} \log f(u).
\end{equation*}
\noindent The $M$-estimator. $\psi = \psi_k$, where $\psi_k$ is the Huber $\psi$-function.  scale estimator is no robust.

knowing $f$

abstract away from the particular choice of $f$; contaminated Gaussian models 

generalization Proposal 2 of \citet{huber1964}  



%------------------------------------------------------------------------------
\chapter{Robust GREG estimator}\label{ch:greg}
Consider the (super-) population model
\begin{equation*}
   \xi: \quad Y_i = \bm x_i^T\bm \beta + E_i \qquad(i=1,\ldots, n),
\end{equation*}
\noindent where 
\begin{enumerate}[i)]
   \item $\bm \beta \in \R^p$ ($p < n$),
   %
   \item $\mathbbm{E}_{\xi}(E_i)=0$,  
   %
   \item $var_{\xi}(E_i) = c_i \sigma^2$ with $\sigma > 0$ and the $c_i$'s are (known) constants such that $c_i > 0$,
   %
   \item $\mathbbm{E}_{\xi}(E_iE_j)=0$ for all $i \neq j$. 
\end{enumerate}

\noindent The parameters $\bm \beta$ and $\sigma^2$ are subject to estimation. Put $\bm X = (\bm x_1^T, \ldots, \bm x_n^T)^T$, where $\bm x_i = (x_{i,1}, \ldots, x_{i,p})^T$. In addition, let $\bm y = (y_1, \ldots, y_n)^T$, $y_i$ denoting the realization of $Y_i$. 

Model $\xi$ can 
\begin{enumerate}[i)]
   \item include a regression intercept (then, we have $x_{i,1} \equiv 1$)  or it can 
   %
   \item be specified as a regression through the origin (RTO).  
\end{enumerate}

Let $\bm b^{(0)}$ denote the weighted least squares (LS) estimate of $\bm \beta$; it serves as our initial estimate. Let  $s^{(0)}$ denote the weighted median of the absolute deviations of the residuals about the weighted median of the residuals (weighted MAD). When $s^{(0)}$ is zero, we compute the weighted interquartile range instead. In contrast to method \texttt{rlm} in package \texttt{MASS}, we do not use the weighted median of the residuals \textit{about zero} as weighted MAD because this scale estimate is not appropriate for the RTO model.\footnote{Under the RTO model, the mean of the residuals is not necessarily equal to zero.} 

Define the function $u:\R \rightarrow \R_+$ given by 
\begin{equation*}
   u(x) = \frac{\psi_k(x)}{x},
\end{equation*}
\noindent where $\psi_k$ denotes the Huber $\psi$-function, and let the residuals be (for any $\bm b \in \R$ and all $i=1,\ldots,n$) 
\begin{equation*}
   r_i(\bm b) = y_i - \bm x_i^T \bm b.
\end{equation*}

Starting with $(\bm \beta^{(0)}, s^{(0)})$, we compute iteratively refined estimates $(\bm \beta^{(\tau)}, s^{(\tau)})$, where $\tau=1,2,\ldots$ counts the number of iterations, 
\begin{align*}
   \bm \beta^{(\tau + 1)} &\leftarrow \text{weighted least squares}\left\{ \mathrm{weights} = w_i\cdot u\left( \frac{r_i\big(\bm \beta^{(\tau)}\big)}{s^{(\tau)}}\right), \bm X, \bm y \right\} \\ 
   %
   s^{(\tau + 1)} &\leftarrow \text{weighted mad}\left\{ \frac{r_i\big(\bm \beta^{(\tau)}\big)}{s^{(\tau)}},\; \mathrm{weights}=w_i \right\} 
\end{align*}
\noindent until the termination rule,
\begin{equation*}
   \big\Vert \bm \beta^{(\tau + 1)} - \bm \beta^{(\tau)} \big\Vert \leq \mathrm{tol} \cdot s^{(\tau + 1)},
\end{equation*}
\noindent is satisfied for some predetermined value $\mathrm{tol}$ (e.g., $\mathrm{tol} = 10^{-5}$). The final result of the iterative updating scheme is denoted by $\widehat{\bm \beta}$ and $\widehat{s}$. 

The joint estimator $(\widehat{\bm \beta}, \, \widehat{s})$ features all the well known properties of an $M$-regression estimator. In particular, $\widehat{s}$ is an affine invariant and scale equivariant estimator of the (super-) population MAD $s$ provied that $\bm \beta^{(0)}$ is a $\sqrt{n}$-consistent, affine and scale equivariant estimator of $\bm \beta$ (this is indeed the case since we use the LS estimator to obtain $\bm \beta^{(0)}$. Under further regularity conditions on the c.d.f. $F$ of the model errors $E_i$, \citet{welsh1986} shows that ---under the assumption that all design weights are unity---$\widehat{s}$ is a $\sqrt{n}$ consistent estimator of $s$. We conjecture that this property generalizes to the case of non-unity sampling weights. Moreover, $\widehat{\bm \beta}$ has asymptotic linear representation in terms of the influence (from which we can easily derive a CLT); see \citet{welsh1986} or \citet[][ch. 5]{jureckovasen1996}.

Before we continue our discussion, we shall address an important property of the joint estimate $(\widehat{\bm \beta}, \, \widehat{s})$. That is, if we let $k \rightarrow \infty$ then the robustness weights are unity for all $n$ observations; hence, estimate $\widehat{\bm \beta}$ is the weighted LS estimate and $\widehat{s}$ is the (normalized) weighted MAD of the LS residuals. This implies that statistical inference for $\bm \beta$ is different ---even if we account for the residual degrees of freedom--- from (classcical) inference statistics for the weighted LS estimate because it is based  on the weighted MAD $\widehat{s}$ instead the estimated residual standard deviation. In order to prevent such an unpleasant situtation, we shall work with a regression estimate of scale other than $\widehat{s}$. To this end, we introduce a close relative of the Huber 'Proposal 2' estimator of scale. This estimator has the nice property that it converges smoothly to the LS regression estimate of scale as $k \rightarrow \infty$.  

We define the robustness weights ($i=1, \ldots, n$)
\begin{equation}\label{eq:regressionscale1}
   u_i = u\left(\frac{r_i(\widehat{\bm \beta})}{\widehat{s}}\right)
\end{equation}
\noindent and compute the following regression estimate of scale
\begin{equation}\label{eq:regressionscale2}
   \widehat{\sigma} = \left[ \frac{1}{\kappa \widehat{N}}\sum_{i=1}^n w_i u_i^2 r_i^2\big(\widehat{\bm \beta}\big) \right]^{1/2}, 
\end{equation}
\noindent where 
\begin{equation*}
   \widehat{N} = \sum_{i=1}^n w_i,
\end{equation*}
\noindent and $\kappa$ is a correction term which ensures that $\widehat{\sigma}$ is a Fisher consistent estimator of $\sigma$ at the Gaussian core model; i.e., $\kappa = \mathbbm{E}\big[\psi_k^2(U)\big]$ where $U \sim N(0,1)$. Note that we define the robustness weights in (\ref{eq:regressionscale1}) in terms of the (weighted) MAD-based estimate of scale, $\widehat{s}$, and the estimated residuals. The estimate of scale $\widehat{\sigma}$ then derives from this. The estimate defined in (\ref{eq:regressionscale2}) does not account for the incurred loss of degrees of freedom; this will be treated later. 


%------------------------------------------------------------------------------
\chapter{GM estimators}
Let $r_i(\bm \theta) = y_i - \bm x_i^T \bm \theta$; $w_i$ is the sampling weight; $h_i$ is the weight in the design space. The class of regression $GM$-estimators of $\bm \theta$ is \citep[Chap. 6.3a][]{hampeletal1986}
\begin{equation}
   \sum_{i \in s} w_i \eta \left(\frac{r_i(\bm \theta)}{\sigma \sqrt{v_i}}, \bm x_i\right)  \frac{\bm x_i}{\sigma \sqrt{v_i}} = \bm 0
\end{equation}
\noindent where 
\begin{equation*}
   \eta^{M}(a,\bm b) = \psi(a) \cdot h(\bm b) \qquad \text{or} \qquad \eta^{S}(a,\bm b) = \psi\left(\frac{a}{h(\bm b)}\right) h(\bm b), 
\end{equation*}
\noindent and $h:\R^p \rightarrow \R_+$ is a weight function.



Let $\bm \theta^{t}$ (and $\sigma^{t}$) denote the trial value at the $t$-th iteration; and updated value, $\bm \theta^{t+1}$, is the root of the weighted LS estimating equation, 
\begin{equation}
   \sum_{i \in s} \overline{\omega}_i \big( \bm \theta^t\big) r_i\big(\bm \theta^{t+1}\big) \bm x_i = \bm 0, 
\end{equation}
\noindent where $\overline{\omega}_i$ is a placeholder for the Mallows and Schweppe weights, respectively, $\overline{\omega}_i^M$ and $\overline{\omega}_i^S$ being defined as  
\begin{equation}
   \overline{\omega}_i^M = \frac{w_i h_i u_i^M}{v_i} \qquad \text{and} \qquad  \overline{\omega}_i^S = \frac{w_i u_i^S}{v_i} 
\end{equation}
\noindent where (supressing the dependency of $u_i^M$, $u_i^S$, and $z_i$ on $\bm \theta$ and $\sigma$) 
\begin{equation}
   u_i^M = \frac{\psi(z_i)}{z_i}, \qquad  u_i^S = \frac{\psi(z_i/h_i)}{z_i/h_i}, \quad \text{and} \quad z_i = \frac{r_i(\bm \theta)}{\sigma \sqrt{v_i}}.
\end{equation}


%------------------------------------------------------------------------------
\section{Covariance matrix}
The asymptotic covariance matrix of the population regression $GM$-estimator $\bm \theta_{GM}$ of $\bm \theta$ (which includes the $M$-estimator as a special case) is 
\begin{equation}\label{eq:cov_pop}
   \mathrm{cov}(\bm \theta_{GM}) = \frac{\sigma^2}{N} \cdot \bm M_{GM}^{-1} \bm Q_{GM} \bm M_{GM}^{-1},
\end{equation}
\noindent where 
\begin{equation*}
   \bm M_{GM} = -\sum_{i=1}^N \mathbbm{E} \left\{\eta' \left( \frac{r_i}{\sigma \sqrt{v_i}}, \bm x_i \right) \right\} \frac{\bm x_i \bm x_i^T}{v_i}, \quad \bm Q_{GM} = \sum_{i = 1}^N \mathbbm{E} \left\{\eta^2 \left( \frac{r_i}{\sigma \sqrt{v_i}}, \bm x_i \right) \right\} \frac{\bm x_i \bm x_i^T}{v_i}.  
\end{equation*}

Next, we consider sample-based estimators of the covariance matrix. Let $\widehat{\bm \theta}_{*}$ denote a placeholder for the estimators $\widehat{\bm \theta}_M$, $\widehat{\bm \theta}_{GM}^{m}$ (Mallows), $\widehat{\bm \theta}_{GM}^{s}$ (Schweppe). The template for the sample estimator of the covariance matrix of $\widehat{\bm \theta}_{*}$ is 
\begin{equation}\label{eq:cov_est}
   \widehat{\mathrm{cov}}(\widehat{\bm \theta}_{*}) = \frac{\widehat{\sigma}^2}{\widehat{N}} \cdot \widehat{\bm M}_{*}^{-1} \widehat{\bm Q}_{*} \widehat{\bm M}_{*}^{-1},
\end{equation}
\noindent where $\widehat{\sigma}$ denotes the (normalized) weighted MAD. Put $\bm W = \mathrm{diag}_{i=1,\ldots,n}\{w_i\}$ for the sampling weights $w_i$, $\bm H = \mathrm{diag}_{i=1,\ldots,n}\{h(\bm x_i)\}$ for the weights in the design space, and define
\begin{equation*}
   \overline{\psi'} = \frac{1}{\widehat{N}}\sum_{i \in s} w_i \psi' \left( \frac{r_i}{\widehat{\sigma} \sqrt{v_i}} \right), \qquad \overline{\psi^2} = \frac{1}{\widehat{N}}\sum_{i \in s} w_i \psi^2 \left( \frac{r_i}{\widehat{\sigma} \sqrt{v_i}} \right).
\end{equation*}
\noindent The covariance matrices of the different estimators of $\bm \theta$ are defined (with the help of the template in Eq. \ref{eq:cov_est}) as follows: 
\begin{enumerate}
   \item{$M$-estimator:}
   \begin{equation*}
      \widehat{\bm M}_M = - \overline{\psi'} \cdot \bm X^T \bm W \bm X, \qquad \widehat{\bm Q}_M = \overline{\psi^2} \cdot \bm X^T \bm W \bm X;
   \end{equation*}
   %
   \item{Mallows $GM$-estimator:} 
   \begin{equation*}
      \widehat{\bm M}_{GM}^{m} = - \overline{\psi'} \cdot \bm X^T \bm W \bm H \bm X, \qquad \widehat{\bm Q}_{GM}^{m} = \overline{\psi^2} \cdot \bm X^T \bm W \bm H^2 \bm X;
   \end{equation*}
   %
   \item{Schweppe $GM$-estimator:} 
   \begin{equation*}
      \widehat{\bm M}_{GM}^{s} = - \bm X^T \bm W \bm S_1 \bm X, \qquad \widehat{\bm Q}_{GM}^{s} = \bm X^T \bm W \bm S_2 \bm X,
   \end{equation*}
   where 
   \begin{equation*}
      S_1 = \mathrm{diag}_{i=1,\ldots,n} \big\{ s_1^i \big\}, \qquad \text{with} \quad s_1^i = \frac{1}{\widehat{N}}\sum_{j \in s} w_j \psi'\left(\frac{r_j}{h(\bm x_i)\widehat{\sigma} \sqrt{v_j}}\right) 
   \end{equation*}
   and 
   \begin{equation*}
      S_2 = \mathrm{diag}_{i=1,\ldots,n} \big\{ s_2^i \big\}, \qquad \text{with} \quad s_2^i = \frac{1}{\widehat{N}}\sum_{j \in s} w_j \psi^2\left(\frac{r_j}{h(\bm x_i)\widehat{\sigma} \sqrt{v_j}}\right).
   \end{equation*}
   Note: The $i$-th diagonal element of $\bm S_1$ and $\bm S_2$ depends on $h(\bm x_i)$, but the summation is over $j \in s$; see also \citet[][Chap. 6]{marazzi1987_2}.
\end{enumerate}

\noindent Huber. Let $z_i = \vert r_j \vert / (k \widehat{\sigma} \sqrt{v_j})$ 
\begin{equation*}
   \bm S_1 = \mathrm{diag}_{i=1,\ldots,n} \big\{ s_1^i \big\}, \quad s_1^i = \sum_{j \in s} w_j \indicator \big\{ h(\bm x_j) \geq z_i \big\}
\end{equation*}




%------------------------------------------------------------------------------
\subsection{Implementation}
To fix notation, put $\bm w =\mathrm{diag}(\bm W)$, $\bm h = \mathrm{diag}(\bm H)$, denote the Hadamard product of the matrices $\bm A$ and $\bm B$ by $\bm A\circ \bm B$ and suppose that $\sqrt{\cdot}$ is applied element by element.

For $M$-estimators, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_m}
   (\bm X^T \bm W \bm X)^{-1}
\end{equation}
\noindent and is computed by the following method:
\begin{enumerate}[i)]
   \item Compute the factorization $\sqrt{\bm w} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Compute $\bm R^{-1} \bm R^{-T}$, which is equal to (\ref{eq:cov_m}); taking advantage of the triangular shape of $\bm R^{-1}$ and $\bm R^{-T}$ (LAPACK: \code{dtrmm}). 
\end{enumerate}

\noindent For the Mallows type $GM$-estimator, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_mallows}
   (\bm X^T \bm W \bm H \bm X)^{-1} \bm X^T \bm W \bm H^2 \bm X (\bm X^T \bm W \bm H \bm X)^{-1}
\end{equation}
\noindent and is computed by the following method:
\begin{enumerate}[i)]
   \item Compute the QR factorization: $\sqrt{\bm w \cdot \bm h} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).   
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Define a new matrix: $\bm A \leftarrow \sqrt{\bm h} \circ \bm Q$ (extraction of $\bm Q$ matrix with LAPACK: \code{dorgqr}).
   %
   \item Update the matrix: $\bm A \leftarrow \bm A \bm R^{-T}$ (taking advantage of the triangular shape of $\bm R^{-1}$; LAPACK: \code{dtrmm}). 
   %
   \item Compute $\bm A \bm A^T$, which corresponds to the expression in (\ref{eq:cov_mallows}); (LAPACK: \code{dgemm}).  
\end{enumerate}

\noindent For the Schweppe type $GM$-estimator, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_schweppe}
   (\bm X^T \bm W \bm S_1 \bm X)^{-1} \bm X^T \bm W \bm S_2 \bm X (\bm X^T \bm W \bm S_1 \bm X)^{-1}.
\end{equation}
\noindent Put $\bm s_1 = \mathrm{diag}(\bm S_1)$, $\bm s_2 = \mathrm{diag}(\bm S_2)$, and let $\cdot / \cdot $ denote elemental division (i.e., the inverse of the Hadamard product). We use the following approach:  
\begin{enumerate}[i)]
   \item Compute the factorization $\sqrt{\bm w \circ \bm s_1} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Define a new matrix: $\bm A \leftarrow \sqrt{\bm s_2 / \bm s_1 } \circ \bm Q$ (extraction of $\bm Q$ matrix with LAPACK: \code{dorgqr}).
   %
   \item Update the matrix: $\bm A \leftarrow \bm A \bm R^{-T}$ (taking advantage of the triangular shape of $\bm R^{-1}$; LAPACK: \code{dtrmm}). 
   %
   \item Compute $\bm A \bm A^T$, which corresponds to the expression in (\ref{eq:cov_schweppe}); (LAPACK: \code{dgemm}).  
\end{enumerate}


\begin{remark}
\citet{marazzi1987_2} uses the Cholesky factorization (see subroutines \code{RTASKV} and \code{RTASKW}) which is computationally a bit cheaper than our QR factorization. 
\end{remark}


%------------------------------------------------------------------------------
% BIB
{
\singlespacing
\bibliographystyle{ecta}
\bibliography{master}
}


%------------------------------------------------------------------------------
\appendix
\chapter{Appendix}

 
\end{document}
