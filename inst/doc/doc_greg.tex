\documentclass[a4paper,oneside,11pt,DIV=12]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\setkomafont{captionlabel}{\sffamily\bfseries\small}
\setkomafont{caption}{\sffamily\small}

\usepackage[T1]{fontenc}
\usepackage{times}
\renewcommand{\familydefault}{\rmdefault}

\usepackage{amssymb,amsmath,amsthm,mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage[longnamesfirst]{natbib}
\usepackage{booktabs}
\usepackage{enumerate}

% base font
\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault}

% theorems
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{rems}{Remarks}

% flush floats using \afterpage{\clearpage}
\usepackage{afterpage}

% allow page breaks of long align equation environments
\allowdisplaybreaks


\usepackage{setspace}
\setlength\parindent{24pt}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% math
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\alert}[1]{\textbf{#1}}

% code command (can deal with '$', '_', etc.)
\makeatletter
\newcommand\code{\bgroup\@makeother\_\@makeother\~\@makeother\$\@makeother\^\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}
\makeatother

% ==============================================================================
\begin{document}
\shortcites{anderson_bai_etal_1999}
\shortcites{blackford_petitet_etal_2002}

\title{\Large Robust Generalized Regression Predictor}

\author{{\normalsize Tobias Schoch} \\
\begin{minipage}[t][][t]{\textwidth}
	\begin{center}
	\small{University of Applied Sciences Northwestern Switzerland FHNW} \\
	\small{School of Business, Riggenbachstrasse 16, CH-4600 Olten} \\
	\small{\texttt{tobias.schoch{@}fhnw.ch}}
	\end{center}
\end{minipage}}

\date{{\small \today}}
\maketitle

\renewenvironment{abstract}{%
\begin{center}\begin{minipage}{0.9\textwidth}
\rule{\textwidth}{0.4pt}
{\sffamily\bfseries\footnotesize Abstract.}\small}
{\par\noindent\rule{\textwidth}{0.4pt}\end{minipage}\end{center}}

%------------------------------------------------------------------------------
\section{Introduction}\label{ch:introduction}
\setstretch{1.1}

The   \emph{population} regression model is given by
\begin{equation}\label{eq:reg_model}
    \xi: \quad Y_i = \bold x_i^T \boldsymbol{\theta} + \sigma \sqrt{v_i}E_i,
    \qquad \boldsymbol{\theta} \in \R^p, \quad \sigma > 0, \quad i \in U,
\end{equation}
\noindent where
\begin{itemize}
    \item the population $U$ is of size $N$,
    \item the parameters $\boldsymbol{\theta}$ and $\sigma$ are unknown,
    \item the $\bold x_i$'s are known values (possibly containing outliers),
        $\bold x_i \in \R^p$, $1 \leq p < N$; we denote the design matrix by
        $\bold X = (\bold x_1, \ldots, \bold x_n)^T$,
    \item the $v_i$'s are known positive (heteroscedasticity) constants,
    \item the errors $E_i$ are independent and identically distributed (i.i.d.)
        random variables (r.v.) with zero expectation and unit variance,
    \item it is assumed that $\sum_{i \in U} \bold x_i \bold x_i^T / v_i$ is a
        non-singular $(p \times p)$ matrix.
\end{itemize}

\begin{rems}
The i.i.d. assumption on the errors $E_i$ is rather strict.
This assumption can be replaced by the assumption that the $E_i$ are
identically distributed r.v. such that $\mathbb{E}_{\xi} (E_i \mid \bold x_i,
\ldots, \bold x_N) = 0$ and $\mathbb{E}_{\xi} (E_i E_j \mid \bold x_i, \ldots,
\bold x_N) = 1$ if $i = j$ and zero otherwise for all $i,j \in U$, where
$\mathbb{E}_{\xi}$ denotes expectation w.r.t. model $\xi$ in
($\ref{eq:reg_model}$). Another generalization obtains by requiring that
$\mathbb{E}_{\xi}(E_i\bold x_i) = \mathbb{E}_{\xi}(\bold x_i E_i) = \bold 0$ in
place of the conditional expectation. If the distribution of the errors $E_i$
is asymmetric with non-zero mean, the regression intercept and the errors are
confounded. The slope parameters, however, are identifiable with asymmetric
distributions \citep{carroll_welsh_1988}. In the context of GREG prediction,
however, we deal with prediction under the model. Thus, identifiability is not
an issue.
\end{rems}

It is assumed that a sample $s$ is drawn from $U$ with sampling design $p(s)$
such that the independence (orthogonality) structure of the model errors in
($\ref{eq:reg_model}$) is maintained. The sample regression $M$- and
$GM$-estimator of $\boldsymbol{\theta}$ are defined as the root to the
following estimating equations \citep[cf.][Chapter
6.3]{hampel_ronchetti_etal_1986}
\begin{align*}
    \sum_{i \in s} \frac{w_i}{\sqrt{v_i}} \psi_k (r_i)\bold x_i &= \bold 0 &
    \text{($M$-estimator)},\\
    \sum_{i \in s} \frac{w_i}{\sqrt{v_i}} h(\bold x_i) \psi_k (r_i) \bold x_i
    &= \bold 0 & \text{(Mallows $GM$-estimator)},\\
    \sum_{i \in s} \frac{w_i}{\sqrt{v_i}} h(\bold x_i)
    \psi_k\left(\frac{r_i}{h(\bold x_i)}\right) \bold x_i &= \bold 0&
    \text{(Schweppe $GM$-estimator)},
\end{align*}
\noindent where
\begin{itemize}
    \item $w_i$ is the sampling weight,
    \item $\psi_k$ is a \emph{generic} $\psi$-function indexed by the robustness
        tuning constant $k$,
    \item $r_i$ is the standardized residual, defined as
    \begin{equation}\label{eq:residuals}
        r_i = \frac{y_i - \bold x_i^T\boldsymbol \theta}{\sigma\sqrt{v_i}},
    \end{equation}
    \item $h:\R^p \rightarrow \R_+ $ is a weight function,
    \item $\sigma$ is the regression scale which is estimated by the
        (normalized) weighted median of the absolute deviations from the
        weighted median of the residuals.
\end{itemize}

\noindent The Huber and Tukey bisquare (biweight) $\psi$-functions are denoted
by, respectively, $\psi_k^{hub}$ and $\psi_k^{tuk}$. The sample-based
estimators of $\boldsymbol \theta$ can be written as a weighted least squares
problem
\begin{equation*}
    \sum_{i \in s} \frac{w_i}{v_i} u_i(r_i,k) (y_i - \bold x_i^T
    \widehat{\boldsymbol \theta}_n) \bold x_i = \bold 0,
\end{equation*}
\noindent where
\begin{equation}\label{eq:ui}
    u_i(r_i,k) = \begin{cases}
        \displaystyle{\frac{\psi_k(r_i)}{r_i}} & \qquad \text{$M$-estimator},\\
        h(\bold x_i)\displaystyle{\frac{\psi_k(r_i)}{r_i}} &
            \qquad \text{Mallows $GM$-estimator},\\
        \displaystyle{\frac{\psi_k(r_i^*)}{r_i^*}}, \quad \text{where} \quad
            r_i^* = \frac{r_i}{h(\bold x_i)} & \qquad \text{Schweppe $GM$-estimator},
    \end{cases}
\end{equation}
\noindent and $k$ denotes the robustness tuning constant.

%------------------------------------------------------------------------------
\section{Representation of the robust GREG as a QR-predictor}
The robust GREG predictor of the population $y$-total can be written in terms
of the $g$-weights \citep[see e.g.][Chapter 6]{sarndal_swensson_etal_1992} as
\begin{equation}\label{eq:gweighted_total}
	\widehat{t}_{y}^{\,rob} = \sum_{i \in s}g_i y_i,
\end{equation}
\noindent where the $g$-weights are defined as \citep{duchesne_1999}
\begin{equation}\label{eq:gweights}
    g_i = b_i + \big(\bold t_{\bold x} - \widehat{\bold t}_{b\bold x}\big)^T
    \left(\sum_{i \in s}q_i \bold x_i \bold x_i^T\right)^{-1}q_i\bold x_i,
\end{equation}
\noindent where $\widehat{\bold t}_{b\bold x} = \sum_{i \in s}b_i \bold x_i$
and $\bold t_{\bold x} = \sum_{i \in U}\bold x_i$. The sampling weights, $w_i$,
are ``embedded'' into the $g$-weights in ($\ref{eq:gweighted_total}$).

In contrast to the non-robust "standard" GREG predictor, the $g$-weights in
($\ref{eq:gweights}$) depend on the study variable, $y_i$, through the choice
of the constants $(q_i, b_i) = \{(q_i,b_i): i \in s\}$. This will be easily
recognized once we define the set of constants. The predictors of the
population $y$-total that are defined in terms of the constants $(q_i,r_i)$
form the class of \emph{QR-predictor} due to \citep{wright_1983}.

\noindent In passing we note that $\widehat{t}_y^{\,rob}$ can be expressed in a
``standard'' GREG representation. Let
\begin{equation*}
    \widehat{\boldsymbol \theta} = \left(\sum_{i \in s}q_i \bold x_i \bold
    x_i^T\right)^{-1} \sum_{i \in s} q_i\bold x_iy_i,
\end{equation*}
\noindent then $\widehat{t}_y^{\,rob}$ in ($\ref{eq:gweighted_total}$) can be
written as
\begin{equation*}
    \widehat{t}_{y}^{\,rob} = \sum_{i \in s} b_i y_i + \big(\bold t_{\bold x} -
    \widehat{\bold t}_{b\bold x}\big)^T \widehat{\boldsymbol \theta} = \bold
    t_{\bold x}^T\widehat{\boldsymbol \theta} + \sum_{i \in s}b_i (y_i - \bold
    x_i^T\widehat{\boldsymbol \theta}).
\end{equation*}
\noindent In the next two sections, we define the constants $(q_i, b_i)$ of the
QR-predictor.

%--------------------------------------
\subsection{Constants $q_i$ of the QR-predictor}
The set of constants $\{q_i\}$ is defined as
\begin{equation}\label{eq:qi}
    q_i =\frac{w_i \cdot u_i(r_i,k)}{v_i}, \qquad i = 1,\ldots,n,
\end{equation}
\noindent where $v_i$ is given in ($\ref{eq:reg_model}$) and $u_i(r_i,k)$ is
defined in ($\ref{eq:ui}$). The tuning constant $k$ in $u_i(r_i,k)$ is the one
that is used to estimate $\boldsymbol \theta$.

%--------------------------------------
\subsection{Constants $b_i$ of the QR-predictor}
The constants $\{b_i\}$ are predictor-specific. They depend on the argument
\code{type}. Moreover, the $b_i$'s depend on the robustness tuning constant
\code{k} -- which is an argument of \code{svymean_reg()} and
\code{svytotal_reg()} -- to control the robustness of the prediction. To
distinguish it from the tuning constant $k$, which is used in fitting model
$\xi$ in ($\ref{eq:reg_model}$), it will be denoted by $\kappa$. Seven sets
$\{b_i\}$ are available.

\begin{description}
    \item[\code{type = "projective":}] $b_i \equiv 0$
        \citep{sarndal_wright_1984},
    \item[\code{type = "ADU":}] $b_i \equiv w_i$ \citep[][Chapter
        6]{sarndal_swensson_etal_1992},
    \item[\code{type = "huber":}] $b_i \equiv w_i \cdot u_i(r_i,\kappa)$, where
        $u_i$ is defined in ($\ref{eq:ui}$) with $\psi_k \equiv \psi_k^{hub}$
        \citep{lee_1995,hulliger_1995,beaumont_alavi_2004},
    \item[\code{type = "tukey":}] $b_i \equiv w_i \cdot u_i(r_i, \kappa)$,
        where $u_i$ is defined in ($\ref{eq:ui}$) with $\psi_k \equiv
        \psi_k^{tuk}$ \citep{lee_1995,hulliger_1995,beaumont_alavi_2004},
    \item[\code{type = "lee":}] $b_i \equiv \kappa \cdot w_i$, where $0 \leq
        \kappa \leq 1$ \citep{lee_1991,lee_1995},
    \item[\code{type = "BR":}] $b_i \equiv w_i \cdot u_i(r_i,\kappa)$, where
        $u_i$ is defined in ($\ref{eq:ui}$) with $\psi_k$ replaced by
        \citep{beaumont_rivest_2009}
        \begin{equation*}
            \psi_k^{mod}(x) = \frac{x}{w_i} + \frac{w_i -
            1}{w_i}\psi_k^{hub}(x),
        \end{equation*}
    \item[\code{type = "duchesne":}] $b_i \equiv w_i \cdot u_i(r_i; a,b)$,
        where $u_i$ is defined in ($\ref{eq:ui}$) with $\psi_k$ replaced by
        \citep{duchesne_1999}
        \begin{equation*}
            \psi_{a,b}^{hub}(x) =
            \begin{cases} x & \text{if} \quad \vert x \vert \leq a, \\ a \cdot
                \mathrm{sign}(x) & \text{if} \quad \vert x \vert > a \quad
                \text{and} \quad \vert x \vert < a/b, \\ b \cdot x & \text{if}
                \quad \vert x \vert > a/b,
            \end{cases}
        \end{equation*}
        where $\psi_{a,b}^{hub}$ is a modified Huber $\psi$-function with
        tuning constants $a$ and $b$ (in place of $\kappa$).
        \citet{duchesne_1999} suggested the default parametrization $a=9$ and
        $b=0.25$.
\end{description}

%--------------------------------------
\subsection{Implementation}
Let $\bold q = (q_1, \ldots, q_n)^T$ and $\bold b = (b_1, \ldots, b_n)^T$,
where $q_i$ and $b_i$ are defined in, respectively, ($\ref{eq:qi}$) and Section
2.2. Put $\bold Z = \sqrt{\bold q} \circ \bold X$, where $\circ$ denotes
Hadamard multiplication and the square root is applied element by element. The
vector-valued $g$-weights, $\bold g = (g_1, \ldots, g_n)^T$, in
($\ref{eq:gweights}$) can be written as
\begin{equation*}
    \bold g^T = \bold b^T + \big(\bold t_{\bold x} - \widehat{\bold t}_{b\bold
    x}\big)^T \underbrace{\big(\bold Z^T \bold Z\big)^{-1} \bold Z^T}_{=\bold
    H, \; \text{say}} \circ (\sqrt{\bold q})^T.
\end{equation*}
\noindent Define the QR factorization $\bold Z = \bold Q \bold R$, where $\bold
Q$ is an orthogonal matrix and $\bold R$ is an upper triangular matrix (both of
conformable size). Note that the matrix QR-factorization and Wright's
QR-estimators have nothing in common besides the name; in particular, $\bold q$
and $\bold Q$ are unrelated. With this we have
\begin{equation*}
	\bold H = \big(\bold Z^T\bold Z\big)^{-1} \bold Z^T =
   \bold R^{-1}\bold Q^T
\end{equation*}
\noindent and multiplying both sides by $\bold R$, we get $\bold R\bold H =
\bold Q^T$ which can be solved easily for $\bold H$ since $\bold R$ is an upper
triangular matrix (see \code{base::backsolve()}). Thus, the $g$-weights can be
computed as
\begin{equation*}
    \bold g = \bold b + \bold H^T \big(\bold t_{\bold x} - \widehat{\bold
    t}_{b\bold x}\big) \circ \sqrt{\bold q},
\end{equation*}
\noindent where the $(p \times n)$ matrix $\bold H$ need not be explicitly
transposed when using \code{base::crossprod()}. The terms $\bold b$ and
$\widehat{\bold t}_{b \bold x}$ are easy to compute. Thus,
\begin{equation*}
    \widehat{t}_y^{\;rob} = \bold g^T\bold y, \qquad \text{where} \quad \bold y
    = (y_1, \ldots, y_n)^T.
\end{equation*}

%------------------------------------------------------------------------------
\section{Variance estimation}
\begin{rem}
Inference of the regression estimator is only implemented under the assumption
of representative outliers (in the sense of Chambers, 1986). We do not
cover inference in presence of nonrepresentative outliers.
\end{rem}

\noindent Our discussion for variance estimation follows the line of reasoning
in \citet[][p. 233--234]{sarndal_swensson_etal_1992} on the variance of the
non-robust GREG estimator. To this end, denote by $E_i = y_i - \bold x_i^T
\boldsymbol \theta_N$, $i \in U$, the census residuals, where $\boldsymbol
\theta_N$ is the census parameter. With this, any $g$-weighted predictor can be
written as
\begin{equation}\label{eq:census_fit}
    \widehat{t}_y^{\,rob} = \sum_{i \in s} g_i y_i = \sum_{i \in s} g_i (\bold
    x_i^T \boldsymbol \theta_N + E_i) = \sum_{i \in U} \bold x_i^T \boldsymbol
    \theta_N + \sum_{i \in s} g_i E_i,
\end{equation}
\noindent where we have used the fact that the $g$-weights in
($\ref{eq:gweights}$) satisfy the calibration property
\begin{equation*}
	\sum_{i \in s} g_i \bold x_i = \sum_{i \in U} \bold x_i.
\end{equation*}
\noindent The first term on the r.h.s. of the last equality in
($\ref{eq:census_fit}$) is a population quantity and does therefore not
contribute to the variance of $\widehat{t}_y^{\,rob}$. Thus, we can calculate
the variance of the robust GREG predictor by
\begin{equation}\label{eq:var_censusfit}
    \mathrm{var}\left(\widehat{t}_y^{\,rob}\right) = \mathrm{var}\left(\sum_{i
    \in s} g_i E_i\right)
\end{equation}
\noindent under the assumptions that (1) the $E_i$ are known quantities and (2)
the $g_i$ do not depend on the $y_i$.

Disregarding the fact that the $g$-weights are sample dependent and
substituting the sample residual $r_i$ for $E_i$ in ($\ref{eq:var_censusfit}$),
\citet[][p. 233-234 and Result 6.6.1]{sarndal_swensson_etal_1992} propose to
estimate the variance of the GREG predictor by the $g$-weighted variance of the
total $\sum_{i \in s}g_ir_i$. Following the same train of thought and
disregarding in addition that the $g_i$ depend on $y_i$, the variance of
$\widehat{t}_y^{\,rob}$ can be approximated by
\begin{equation*}
    \widehat{\mathrm{var}}\big(\widehat{t}_y^{\,rob}\big) \approx
    \widehat{\mathrm{var}}\left(\sum_{i \in s}g_i r_i\right),
\end{equation*}
\noindent where $\widehat{\mathrm{var}}(\cdot)$ denotes a variance estimator of
a total for the sampling design $p(s)$.

\clearpage
\bibliographystyle{fhnw_en}
\bibliography{master}

\end{document}
