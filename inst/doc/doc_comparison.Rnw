% in R: Sweave("doc_comparison.Rnw")
%


\documentclass[a4paper,oneside,11pt,DIV=12]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\setkomafont{captionlabel}{\sffamily\bfseries\small}
\setkomafont{caption}{\sffamily\small}

\usepackage[T1]{fontenc}
\usepackage{times}
\renewcommand{\familydefault}{\rmdefault}

\usepackage{amssymb,amsmath,amsthm,mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage[longnamesfirst]{natbib}
\usepackage{booktabs}
\usepackage{enumerate}

% base font
\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault}

% theorems
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{rems}{Remarks}

% flush floats using \afterpage{\clearpage}
\usepackage{afterpage}

% allow page breaks of long align equation environments
\allowdisplaybreaks


\usepackage{setspace}
\setlength\parindent{24pt}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% math
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\alert}[1]{\textbf{#1}}

% code command (can deal with '$', '_', etc.)
\makeatletter
\newcommand\code{\bgroup\@makeother\_\@makeother\~\@makeother\$\@makeother\^\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}
\makeatother

% Sweave
\usepackage{Sweave}
<<Sweave settings, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 80, useFancyQuotes = FALSE)
options(SweaveHooks = list(fig = function()par(mar = c(4.5, 4, 1, 2))))
library(robsurvey, quietly = TRUE)
library(survey)
library(MASS)
library(robustbase)
library(robeth)
source("comparison_functions.R")
@


% ==============================================================================
\begin{document}
\shortcites{anderson_bai_etal_1999}
\shortcites{blackford_petitet_etal_2002}

\title{\Large Comparison With Other Implementations of Regression $M$- and
    $GM$-Estimators}

\author{{\normalsize Tobias Schoch} \\
\begin{minipage}[t][][t]{\textwidth}
	\begin{center}
	\small{University of Applied Sciences Northwestern Switzerland FHNW} \\
	\small{School of Business, Riggenbachstrasse 16, CH-4600 Olten} \\
	\small{\texttt{tobias.schoch{@}fhnw.ch}}
	\end{center}
\end{minipage}}

\date{{\small \today}}
\maketitle

\renewenvironment{abstract}{%
\begin{center}\begin{minipage}{0.9\textwidth}
\rule{\textwidth}{0.4pt}
{\sffamily\bfseries\footnotesize Abstract.}\small}
{\par\noindent\rule{\textwidth}{0.4pt}\end{minipage}\end{center}}

\begin{abstract}
In this report, we study the behavior of the methods \code{svyreg_huberM} and
\code{svyreg_huberGM} in package \code{robsurvey} with other
implementations. We restricted attention to studying the methods for 4
well-known datasets. For all datasets under study, our implementations are
identical (in terms of floating point arithmetic) with results of the
competing implementations. Although our comparisons provide only anecdotal
evidence on the performance of the methods, we believe that the comparisons
shed some light on the behavior of our implementations. We are fairly
confident that the methods in package \code{robsurvey} behave the way they
are supposed to.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}\label{ch:introduction}
\setstretch{1.1}
In this short report, we compare the behavior of the regression $M$- and
$GM$-estimators in package \code{robsurvey} with the methods from other
implementations. To this end, we study the estimated parameters for four
well-known datasets/ cases studies. With regard to competing implementations,
we consider the methods from the following R packages:

<<echo=FALSE>>=
pkg <- c("MASS", "robeth")
for (i in 1:length(pkg))
    cat(paste0(pkg[i], ", version: ", packageVersion(pkg[i]),"\n"))
@
\noindent These packages are documented in, respectively,
\citep{venables_ripley_2002} and \citet{marazzi_2020}. The datasets are from
package

<<echo=FALSE>>=
pkg <- "robustbase"
cat(paste0(pkg, ", version: ", packageVersion(pkg),"\n"))
@
\noindent see \citet{maechler_rousseeuw_etal_2022}. In all comparisons, we
\begin{itemize}
    \item study $M$- or $GM$-estimators with the MAD (normalized median
        absolute deviation) as estimator of scale;
  \item use the robustness tuning constant \code{k = 1.345} of the Huber
      $\psi$-function;
    \item focus on sample data that do not contain sampling weights.
\end{itemize}

\noindent All studied methods compute the regression estimates by iteratively
reweighted least squares (IRWLS) and the estimate of scale (more precisely, the
trial value for the scale estimate) is updated at each iteration.

\begin{rem}
Our comparisons provide only anecdotal evidence on the performance of the
methods. Nonetheless, we believe that the comparisons shed some light on
the behavior of our implementations.
\end{rem}

\noindent Let $\bm x$ and $\bm y$ denote two real-valued $p$-vectors. We define
the absolute relative difference by
\begin{equation*}
    \mathrm{abs\_rel\_DIFF}(\bm x, \bm y) = 100\% \cdot \max_{i=1, \ldots, p}
    \left\{ \Big\vert \frac{x_i}{y_i} - 1 \Big\vert \right\}.
\end{equation*}

\noindent The remainder of the paper is organized as follows. In Section
\ref{sec:m_reg}, we compare several implementations of the Huber $M$-estimator
of regression.  Section \ref{sec:gm_reg} studies implementations of the Huber
$GM$-estimator of regression. In Section 4, we summarize the findings.

%------------------------------------------------------------------------------
\section{Huber $M$-estimators of regression}\label{sec:m_reg}
In this section, we study the Huber $M$-estimator of regression. The
parametrizations of the algorithms have been chosen to make them comparable; we
use:
\begin{itemize}
    \item \code{MASS::rlm}: \code{method = "M"}, \code{scale.est = "MAD"},
        ~\code{acc = 1e-5},
        ~\code{test.vec = "coef"},
        and \code{maxit = 50},
    \item \code{robeth::rywalg}: \code{tol = 1e-5},
        ~\code{maxit = 50},
        ~\code{itype = 1},
        ~\code{isigma = 2},
        ~\code{icnv = 1},
        and \code{maxis =
        1}; see \citet{marazzi_1993} for more details.
    \item \code{robsurvey::svyreg_huberM}: \code{tol = 1e-5}, and \code{maxit =
        50}.
\end{itemize}
\noindent The methods \code{MASS::rlm} and \code{robeth::rywalg} compute the
regression scale estimate by the (normalized) median of the absolute deviations
(MAD) \emph{about zero}. The method \code{svyreg_huberM} (and
\code{svyreg_tukeyM}) implements two variants of the MAD:
\begin{itemize}
    \item \code{mad_center = FALSE}: MAD centered about zero,
    \item \code{mad_center = TRUE}: MAD centered about the (weighted) median.
        (This is the default).
\end{itemize}
\noindent For ease of reference, we denote the MAD centered about zero by
\code{mad0}.

In practice, the estimate of regression and scale differ whether the MAD is
centered about zero or the median because the median of the residuals is not
exactly zero for empirical data. If the residuals have a skewed distribution,
the two variants of the MAD can differ by a lot.

%---------------------------------------
\subsection{Case 1: education data}
The \code{education} data are on public education expenditures (at the level of
US states), and are from \citet{chatterjee_price_1977} [see
\citet{chatterjee_hadi_2012} for a newer edition]; see also
\citet{rousseeuw_leroy_1987}. The dataset contains 4 variables: the response
variable (\code{Y}: per capita expenditure on public education in a state,
projected for 1975) and the three explanatory variables

\begin{itemize}
    \item \code{X1}: Number of residents per thousand residing in urban areas
        in 1970,
    \item \code{X2}: Per capita personal income in 1973,
    \item \code{X3}: Number of residents per thousand under 18 years of age in
        1974.
\end{itemize}
\noindent The following tabular output shows the estimated coefficients (and
the estimated scale; last column) under the model \code{Y ~ X1 + X2 + X3} for 4
different implementations/ methods.

<<>>=
data(education, package = "robustbase")
M_compare(Y ~ X1 + X2 + X3, education)
@

\noindent The estimates of the 4 methods differ only slightly. We have the
following findings:

\begin{itemize}
    \item \code{svyreg_huberM}~\code{(mad0)} is based on the MAD centered about
        zero.  In methodological terms, it is identical with the
        implementations \code{rlm}~\code{(MASS)} and
        \code{rywalg}~\code{(ROBETH)}. The estimates of
        \code{svyreg_huberM}~\code{(mad0)} are virtually identical with the
        ones of \code{rlm}~\code{(MASS)}. The estimates of
        \code{rywalg}~\code{(ROBETH)} deviate more from the other methods.
    \item \code{svyreg_huberM} is based on the MAD centered about the
        (weighted) median. The estimates differ slightly from
        \code{svyreg_huberM}~\code{(mad0)}.
\end{itemize}

The discrepancies are mainly due to the normalization constant to make the MAD
an unbiased estimator of the scale at the Gaussian core model. In \code{rlm
(MASS)}, the MAD about zero is computed by \code{median(abs(resid)) / 0.6745}.
The constant $1 / 0.6745$ is equal to $1.482580$ (with a precision of 6 decimal
places), which differs slightly from $1/\Phi^{-1}(0.75)=1.482602$, where $\Phi$
denotes the cumulative distribution function of the standard Gaussian.  The
implementation of \code{svyreg_huberM} uses $1.482602$ (see file
\code{src/constants.h}). Now, if we replace $1 / 0.6745$ in the above code
snippet by $1.482602$ in the function body of \code{rlm.default}, then the
regression coefficients of the so modified code and \code{svyreg_huberM} are
(in terms of floating point arithmetic) almost identical. The absolute relative
difference is

<<>>=
design <- svydesign(id = ~1, weights = rep(1, nrow(education)),
                    data = education)
m1 <- svyreg_huberM(Y ~ X1 + X2 + X3, design, k = 1.345,
                    mad_center = FALSE, tol = 1e-5,
                    maxit = 50)

rlm_mod <- MASS:::rlm.default

body(rlm_mod)[[22]][[4]][[3]][[3]][[2]][[3]][[3]][[3]] <-
    substitute(median(abs(resid)) * 1.482602)

m2 <- rlm_mod(m1$model$x, m1$model$y, k = 1.345,
              method = "M", scale.est = "MAD", acc = 1e-5,
              maxit = 50, test.vec = "coef")

cat("\nabs_rel_DIFF: ", 100 * max(abs(coef(m1) / coef(m2) - 1)),
    "%\n")
@

\noindent Next, we consider comparing the estimated (asymptotic) covariance
matrix of the estimated regression coefficients. To this end, we computed the
diagonal elements of the estimated covariance matrix for the methods
\code{svyreg_huberM (mad0)} and \code{rlm (MASS)}; see below. In addition, we
computed the absolute relative difference between the two methods.

<<>>=
M_compare_cov(Y ~ X1 + X2 + X3, education)
@

\noindent The diagonal elements of the estimated covariance matrix differ only
slightly between the two methods. The discrepancies can be explained by the
differences in terms of the estimated coefficients.

%---------------------------------------
\subsection{Case 2: stackloss data}
The \code{stackloss} data consist of 21 measurements on the oxidation of
ammonia to nitric acid for an industrial process; see \citet{brownlee_1965}.
The variables are:
\begin{itemize}
    \item \code{Air Flow}: flow of cooling air,
    \item \code{Water Temp}: cooling water inlet temperature,
    \item \code{Acid Conc.}: concentration of acid [per 1000, minus 500],
    \item \code{stack.loss}: stack loss.
\end{itemize}
\noindent The variable \code{stack.loss} (stack loss of amonia) is regressed on
the explanatory variables air flow, water temperature and the concentration of
acid. The regression coefficients and the estimate of scale are tabulated for
the 4 implementations/ methods under study.

<<>>=
data(stackloss, package = "datasets")
M_compare(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc.,
          stackloss)
@

\noindent The estimates of the regression $M$-estimator which is based on the
MAD centered about zero are virtually identical (see rows 2--4). The estimates
of \code{svyreg_huberM} deviate slightly from the latter because it is based on
the MAD centered about the (weighted) median.

\noindent We did not repeat the analysis on differences in the estimated
covariance matrices because the results are qualitatively the same as in Case
1.

%------------------------------------------------------------------------------
\section{Huber $GM$-estimators of regression}\label{sec:gm_reg}
In this section, we consider regression $GM$-estimators with Huber
$\psi$-function (tuning constant fixed at $k=1.345$). The scale is estimated by
MAD. With regard to the MAD, we distinguish two cases: \code{svyreg_huberGM}
and \code{svyreg_huberGM (mad0)}, where \code{mad0} refers to the MAD about
zero.

We computed the weights to downweight leverage observations (\code{xwgt}) with
the help of the methods in package \code{robeth}.  The so computed weights were
then stored to be utilized in all implementations of $GM$-estimators of
regression. This approach ensures that the implementations do not differ in
terms of the \code{xgwt}'s.

%---------------------------------------
\subsection{Case 3: delivery data}
The \code{delivery} data consist of observations on servicing 25 soft drink
vending machines. The data are from \citet{montgomery_peck_2006}; see also
\citet{rousseeuw_leroy_1987}. The variables are:

\begin{itemize}
    \item \code{n.prod}: number of products stocked in the vending machine,
    \item \code{distance}: distance walked by the route driver (ft),
    \item \code{delTime}: delivery time (minutes).
\end{itemize}

\noindent The goal is to model/ predict the amount of time required by the
route driver to service the vending machines.  The variable \code{delTime} is
regressed on the variables \code{n.prod} and \code{distance}.


\subsubsection*{Mallows $GM$-estimator}
The regression coefficients and the estimate of scale are tabulated for the 3
implementations/ methods under study.

<<>>=
data(delivery, package = "robustbase")
GM_mallows_compare(delTime ~ n.prod + distance, delivery)
@

\noindent The estimates of \code{svyreg_huberGM}~\code{(Mallows,}~\code{mad0)}
are almost identical with results of
\code{rywalg}~\code{(ROBETH,}~\code{Mallows)}; see rows 2 and 3.  The estimates
of \code{svyreg_huberGM}~\code{(Mallows)} (i.e., based on the MAD centered about
the weighted median differ slightly as is to be expected.

\subsubsection*{Schweppe $GM$-estimator}

<<>>=
GM_schweppe_compare(delTime ~ n.prod + distance, delivery)
@

\noindent The estimates of \code{svyreg_huberGM}~\code{(Schweppe, mad0)} and
\code{rywalg}~\code{(ROBETH,}~\code{Schweppe)} (see rows 2 and 3) are slightly
different. We could not figure out the reasons for this discrepancy.

%---------------------------------------
\subsection{Case 4: salinity data}
The \code{salinity} data are a set of measurements of water salinity and river
discharge taken in North Carolina's Pamlico Sound;
\citet{ruppert_carroll_1980}; see also \citet{rousseeuw_leroy_1987}. The
variables are

\begin{itemize}
    \item \code{Y}: salinity,
    \item \code{X1}: salinity lagged two weeks,
    \item \code{X2}: linear time trend,
    \item \code{X3}: river discharge.
\end{itemize}

\noindent There a 28 observations. We consider fitting the model \code{Y ~ X1 +
X2 + X3} by several implementations of the regression $GM$-estimators.

\subsubsection*{Mallows $GM$-estimator}

<<>>=
data(salinity, package = "robustbase")
GM_mallows_compare(Y ~ X1 + X2 + X3, salinity)
@

\noindent The differences between the estimates of \code{svyreg_huberGM
(Mallows, mad0)} and \code{rywalg (ROBETH, Mallows)} are larger (see rows 2 and
3) than in Case 3. Still, the estimates are very similar.


\subsubsection*{Schweppe $GM$-estimator}

<<>>=
GM_schweppe_compare(Y ~ X1 + X2 + X3, salinity)
@

\noindent The estimates of \code{svyreg_huberGM}~\code{(Schweppe,}~\code{mad0)}
and \code{rywalg}~\code{(ROBETH,}~\code{Schweppe)} (see rows 2 and 3) are
slightly different. But the differences are minor.

%------------------------------------------------------------------------------
\section{Summary}\label{sec:summary}
In this paper, we studied the behavior of the methods \code{svyreg_huberM} and
\code{svyreg_huberGM} in package \code{robsurvey} with other implementations.
We restricted attention to studying the methods for four well-known datasets.
For all datasets under study, our implementations replicate (or are at least
very close to) the results of the competing implementations. Although our
comparisons provide only anecdotal evidence on the performance of the methods,
we believe that the comparisons shed some light on the behavior of our
implementations.


%------------------------------------------------------------------------------
\clearpage
\bibliographystyle{fhnw_en}
\bibliography{master}

\end{document}
