\documentclass[a4paper,oneside,11pt,DIV=12]{scrartcl}

\setkomafont{captionlabel}{\sffamily\bfseries\small}
\setkomafont{caption}{\sffamily\small}

\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\usepackage{times}

\usepackage{amsmath,amsfonts,amssymb,bm,bbm,amsthm,mathrsfs}
\newcommand{\indicator}{{\mathbbm 1}}   % Indicator function
\newcommand{\R}{{\mathbbmss R}}         % real values
\newcommand{\code}[1]{{\texttt{#1}}}    % code

\usepackage{booktabs}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{remarks}{Remarks}

\usepackage{setspace}
\usepackage[longnamesfirst]{natbib}

\setlength\parindent{24pt}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% ==============================================================================
\begin{document}
\shortcites{robustbase}

\title{Documentation of the Implemented Methods}

\author{{\normalsize Tobias Schoch} \\
\begin{minipage}[t][][t]{\textwidth}
    \begin{center}
    \small{University of Applied Sciences Northwestern Switzerland FHNW} \\
    \small{School of Business, Riggenbachstrasse 16, CH-4600 Olten} \\
    \small{\texttt{tobias.schoch{@}fhnw.ch}}
    \end{center}
\end{minipage}}

\date{{\small \today}}
\maketitle

\renewenvironment{abstract}{%
\begin{center}\begin{minipage}{0.9\textwidth}
\rule{\textwidth}{0.4pt}
{\sffamily\bfseries\footnotesize Abstract.}\small}
{\par\noindent\rule{\textwidth}{0.4pt}\end{minipage}\end{center}}

\begin{abstract}
In this note, we explain how to add support for additional $\psi$-functions
for $M$- and $GM$-estimators of regression.
\end{abstract}

\vspace{1em}

\setstretch{1.2}

%------------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}


\subsection{Notation and Preliminaries}
sample $s$ of size $n$ from $U$


Consider the study variable $y_i$, $i \in U$. The population distribution function of $y_i$ is denoted by $F_{y,N}$ or just $F_N$ for notational convenience (i.e., the dependence on $y$ is suppressed),
\begin{equation*}
   F_N(y) = \frac{1}{N}\sum_{i \in U} \indicator\{y_i \leq y \},
\end{equation*}
\noindent where $\indicator\{ A \}$ is the indicator function which is equal to one if condition $A$ holds and zero otherwise. A sample-based estimate is given by
\begin{equation*}
   \widehat{F}(y) = \frac{1}{\widehat{N}}\sum_{i \in s} w_i \indicator\{y \leq y_i \} \qquad \text{with} \qquad \widehat{N} = \sum_{i \in s} w_i,
\end{equation*}
\noindent where $w_i$ is the sampling weight. Although estimator $\widehat{F}$ may be biased as an estimator of $F_N$, it is consistent \citep[][]{sarndal_swensson_etal_1992}.

Let $Q_y(p)$ denote the $p$th population quantile function, where $p \in [0,1]$. We use the shorthand notation $Q_p$ whenever no confusion can arise. Formally, $Q_y(p)$ is defined as (for $0 \leq p \leq 1$)
\begin{equation*}
   Q_p = \inf \{ y : F_N(y) \geq p \}.
\end{equation*}
\noindent A sample-based estimate of the population quantile $Q_p$ is denoted by $\widehat{Q}_p$.

The indicator function is denoted by $\indicator\{A\}$ which is equal to one if $A$ is true and zero otherwise.

%------------------------------------------------------------------------------
\section{Basic Robust Estimators}\label{sec:basic}

Let $J: \R \rightarrow [0,1]$ be a function and define the $L$-functional
\begin{equation}
    T(F) = \int y J\big(F(y)\big)\mathrm{d}F(y)
\end{equation}
\noindent for any c.d.f. $F$, where integration is over the domain of $y$. An $L$-estimator is generally of the from \citep{shao_1994}
\begin{equation}\label{eq:l_est}
    \widehat{T} = T(\widehat{F}) + \sum_{k = 1}^K \alpha_k \widehat{Q}_{p_k},
\end{equation}
\noindent where $K$ is a fixed integer, $\alpha_k$ and $p_k$ are constants ($0 < p_k < 1$ for all $k$). If $\alpha_k \equiv 0$, $\widehat{T}$ is called a smooth $L$-statistic. If $J \equiv 0$, then $\widehat{T}$ is linear combination of sample quantiles (order statistics). Note: \citet{shao_1994} defined $\widehat{F}$ slightly different.

%------------------------------------------------------------------------------
\subsection{Weighted trimmed mean and total}
Let $0 \leq \alpha < \beta \leq 1$. The $(\alpha, \beta)$-trimmed weighted estimator of the population mean is defined as
\begin{equation*}
  \widehat{m}_y^{\alpha,\beta, trim} = \frac{\sum_{i \in s} w_i y_i \indicator \{\widehat{Q}_{\alpha} \leq y_i \leq \widehat{Q}_{\beta} \}}{(\beta - \alpha)\sum_{i \in s}w_i}.
\end{equation*}
\noindent It can written as a smooth $L$-estimator (see Eq. \ref{eq:l_est}) with $\alpha_k \equiv 0$ and $J(u) = (\beta - \alpha)^{-1} \indicator\{ \alpha \leq u \leq \beta \}$. \citet[][Thm. 1 and 2]{shao_1994} proves that---granted some regularity conditions---$\{T(\widehat{F}) - T(F)\} / \sigma$ has an asymptotic standard normal distribution, where $\sigma^2$ is the asymptotic variance (w.r.t. randomization distribution),
\begin{equation*}
    \sigma^2 = \mathrm{var} \left( \sum_{i \in s} w_i z_i \right),
\end{equation*}
\noindent and $z_i$ denotes the value of the influence function (IF) of $T$ evaluated at $y_i$. The IF of the trimmed mean is 



%------------------------------------------------------------------------------
\subsection{Weighted winsorized mean and total}





%------------------------------------------------------------------------------
\subsection{Box: Trimmed and winsorized mean}
Consider $n$ independent and indentically real-valued r.v.'s $Y_i$ ($i=1,\ldots,n$) with cdf $F$. Let $\alpha \in [0, 0.5)$ be a fixed proportion. The $\alpha$-trimmed sample $y$-mean, $\widehat{T}$, and the statistical functional corresponding to the trimmed mean, $T(F)$, are defined by, respectively,
\begin{equation*}
   \widehat{T} = \frac{1}{n-2\lfloor \alpha n\rfloor}\sum_{i=\lfloor \alpha n\rfloor + 1}^{n - \lfloor \alpha n\rfloor}y_{(i:n)} \qquad  \text{and} \qquad  T(F) = \frac{1}{1 - 2\alpha} \int_{\alpha}^{1-\alpha} u \mathrm{d} F^{-1}(u),
\end{equation*}
\noindent where $y_{(i:n)}$ is the $i$th order statistic, $F^{-1}$ is the quantile function, and $\lfloor u \rfloor$ denotes the greatest integer less than or equal to $u$. Under very general conditions on $F$ (see Stigler, 1973), the large sample distribution of $\sqrt{n}(\widehat{T} - T)$ is normal with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(T,y_i)/n$, where $\mathrm{IF}$ is the influence function of $T$ and is given by (for any $z \in \R$)
\begin{align*}
   \mathrm{IF}\big(T, z\big) = \frac{1}{1-2\alpha}
   \begin{cases}
      F^{-1}(\alpha) - W(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - W(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      % 
      F^{-1}(1-\alpha) - W(F) & \text{if} \quad z > F^{-1}(1-\alpha), 
   \end{cases}
\end{align*}
\noindent where
\begin{equation}\label{eq:functionalwinsorized}
   W(F) = (1 - 2\alpha )T(F) + \alpha F^{-1}(\alpha) + \alpha F^{-1}(1-\alpha)
\end{equation}
\noindent is the functional corresponding to the $\alpha$-winsorized mean; see e.g. \citet[][p. 58]{huber_1981}. An estimate of $W$, say $\widehat{W}$, obtains by replacing the quantiles $F^{-1}(\alpha)$ and $F^{-1}(1-\alpha)$ in (\ref{eq:functionalwinsorized}) with sample-based quantiles and substituting $\widehat{T}$ for $T$. Estimator $\widehat{W}$ has (when properly scaled) a limiting normal distribution with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(W,y_i) / n$, where \citep[see e.g.][58--59]{huber_1981}
\begin{align*}
   \mathrm{IF}\big(W, z\big) = 
   \begin{cases}
      F^{-1}(\alpha) - \frac{\alpha}{f\big(F^{-1}(\alpha)\big)} - C(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - C(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(1-\alpha),\\
      % 
      F^{-1}(1-\alpha) + \frac{\alpha}{f\big(F^{-1}(1-\alpha)\big)}- C(F) & \text{if} \quad z > F^{-1}(1-\alpha),
   \end{cases}
\end{align*}
\noindent where functional $C$ is defined as
\begin{equation*}
   C(F) = W(F) - \frac{\alpha^2}{f\big(F^{-1}(\alpha)\big)} - \frac{\alpha^2}{f\big(F^{-1}(1-\alpha)\big)}.
\end{equation*}
\noindent Observe that the influence function has jumps at $F^{-1}(\alpha)$ and $F^{-1}(1-\alpha)$ and that it depends on the probability density function $f$.




%------------------------------------------------------------------------------
\subsection{Variances}
r.v. $Y$ defined on $\R$ with cdf $F$. The trimmed mean is defined as $(\alpha,\beta)$ where $0 \leq \alpha < 1- \beta \leq 1)$
\begin{equation*}
   \widehat{T} = \frac{1}{n - \lfloor \alpha n\rfloor - \lfloor \beta n\rfloor}\sum_{i= \lfloor \alpha n \rfloor + 1}^{n - \lfloor \beta n \rfloor} y_i,
\end{equation*}
\noindent where $\lfloor u \rfloor$ denotes the greatest integer less than or equal to $u$. The statistical functional corresponding to the trimmed mean at the cdf $F$ is
\begin{equation*}
   T(F) = \frac{1}{1 - \alpha - \beta} \int_{\alpha}^{1-\beta} u \mathrm{d} F^{-1}(u),
\end{equation*}
\noindent where $F^{-1}$ is the inverse of distribution function $F$. Under very general conditions on $F$ (see Stigler, 1973), the large sample distribution of $\sqrt{n}(\widehat{T} - T)$ is normal with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(T,y_i)/n$, where $\mathrm{IF}$ is the influence function of $T$ and is given by (for any $z \in \R$)
\begin{align*}
   \mathrm{IF}\big(T, z\big) = \frac{1}{1-\alpha - \beta}
   \begin{cases}
      F^{-1}(\alpha) - W(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - W(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      % 
      F^{-1}(1-\beta) - W(F) & \text{if} \quad z > F^{-1}(1-\beta),
   \end{cases}
\end{align*}
\noindent where
\begin{equation}\label{eq:functionalwinsorized}
   W(F) = (1 - \alpha -\beta)T + \alpha F^{-1}(\alpha) + \beta F^{-1}(1-\beta)
\end{equation}
\noindent is the functional corresponding to the $(\alpha,\beta)$-winsorized mean; see e.g. \citet[][p. 58]{huber_1981}. An estimate of $W$, say $\widehat{W}$, obtains by replacing the quantiles $F^{-1}(\alpha)$ and $F^{-1}(1-\beta)$ in (\ref{eq:functionalwinsorized}) with sample-based quantiles and substituting $\widehat{T}$ for $T$. Estimator $\widehat{W}$ has (when properly scaled) a limiting normal distribution with mean 0 and asymptotic variance $\sum_{i=1}^n \mathrm{IF}^2(W,y_i) / n$, where \citep[see e.g.][58--59]{huber_1981}
\begin{align*}
   \mathrm{IF}\big(W, z\big) =
   \begin{cases}
      F^{-1}(\alpha) - \frac{\alpha}{f\big(F^{-1}(\alpha)\big)} - C(F) & \text{if} \quad x < F^{-1}(\alpha),\\
      %
      z - C(F) & \text{if} \quad F^{-1}(\alpha) \leq z \leq F^{-1}(\beta),\\
      %
      F^{-1}(1-\beta) + \frac{\beta}{f\big(F^{-1}(1-\beta)\big)}- C(F) & \text{if} \quad z > F^{-1}(1-\beta),
   \end{cases}
\end{align*}
\noindent where functional $C$ is defined as
\begin{equation*}
   C(F) = W(F) - \frac{\alpha^2}{f\big(F^{-1}(\alpha)\big)} - \frac{\beta^2}{f\big(F^{-1}(1-\beta)\big)}.
\end{equation*}
\noindent Observe that the influence function has jumps at $F^{-1}(\alpha)$ and $F^{-1}(1-\beta)$ and that it depends on the probability density function $f$.



%------------------------------------------------------------------------------
\section{Regression $M$- and $GM$-estimators}
Consider the linear (super-) population model
\begin{equation}\label{eq:reg_model}
    Y_i = \bm x_i^T \bm \theta + \sigma \sqrt{v_i}E_i, \qquad \bm \theta \in \R^p, \quad \sigma > 0, \quad i \in U,
\end{equation}
\noindent where
\begin{enumerate}[i)]
    \item the parameters $\bm \theta $ and $\sigma$ are unknown;
    \item the $\bm x_i$'s are known values (possibly containing outliers), $\bm x_i \in \R^p$, $1 \leq p < \mathrm{card}(U)$;
    \item the $v_i$'s are known positive (heteroscedasticity) constants;
    \item the errors $E_i$ are independent and identically distributed (i.i.d.) random variables (r.v.) with c.d.f. $F$, zero expectation, and unit variance, formally $\mathbbm{E}_{\xi}(E_i) = 0$ and $\mathbbm{E}_{\xi}(E_i^2)=1$ for all $i\in U$ [$\mathbbm{E}_{\xi}$ refers to expectation w.r.t. the model; we write $\mathbbm{E}_p$ to mean expectation w.r.t. the randomization distribution or sampling design];
    \item it is assumed that $\sum_{i \in U} \bm x_i \bm x_i^T / v_i$ is a non-singular $(p \times p)$ matrix;
    \item the realizations of the r.v.'s $Y_i$ and $E_i$ are denoted by, respectively, $y_i$ and $e_i$.
\end{enumerate}

\begin{remark}
The independence assumption on the $E_i$'s is plausible with single-stage and elemental sampling designs $p(s)$, but less so for clustered multistage designs; see \citet[][Chap. 7]{sarndal_swensson_etal_1992}. We restrict attention to the super-population model in (\ref{eq:reg_model}) which is to be sampled element by element such that the independence structure of the errors is maintained.
\end{remark}

The \textit{population} regression $GM$-estimator of $\bm \theta$ is defined as the root $\bm \theta_{N}$ of the (system of) estimating functions $\bm \Psi_N(\bm \theta_N, \sigma) = \bm 0$ for a given $\sigma > 0$, where \citep[][Chap. 6.3]{hampel_ronchetti_etal_1986}
\begin{equation}\label{eq:gm_ee}
    \bm \Psi_N(\bm \theta, \sigma) = \sum_{i=1}^N \bm \Psi_i(\bm \theta, \sigma)
\end{equation}
with
\begin{equation}\label{eq:psi_gm}
    \bm \Psi_i(\bm \theta, \sigma) = \eta\left(\frac{y_i - \bm x_i^T \bm \theta}{\sigma \sqrt{v_i}}, \; \bm x_i\right) \frac{\bm x_i}{\sigma \sqrt{v_i}},
\end{equation}
where the function $\eta: \R \times \R^p \rightarrow \R$ parametrizes quite a number of candidate $M$- and $GM$-estimators. The best renowned estimators are due to, respectively, C.L. Mallows and F.C. Schweppe, and are defined (together with the regression $M$-estimator) as
\begin{center}
\begin{tabular}{lllll}
    $M$-estimator: && Mallows type $GM$-estimator: && Schweppe type $GM$-estimator: \\
    \addlinespace
    $\eta(r,\bm x) = \psi(r)$ && $\eta(r,\bm x) =\psi(r) \cdot h(\bm x)$ && $\eta(r,\bm x) = \displaystyle{\psi\left(\frac{r}{h(\bm x)}\right)} \cdot h(\bm x)$
\end{tabular}
\end{center}
where $\psi:\R \rightarrow \R$ is a continuous, bounded, and odd (possibly redescending) function, and $h: \R^p \rightarrow \R_+$ is a weight function. To keep the notation clear, we do not introduce separate notation for the $M$- and the Mallows and Schweppe type $GM$-estimators. Instead, we use the generic notation $\bm \theta_N$. The scale $\sigma$ in $\bm \Psi_i(\bm \theta, \sigma)$ ensures that the $GM$-estimator is a scale equivariant estimator of $\bm \theta$. In applications, we estimate the tuple $(\bm \theta_{N}, \sigma)$ jointly.

The influence function (IF) of $\bm \theta_{N}$---regarded as a statistical regression functional $\bm \theta (F)$at the distribution $F$---evaluated at $(y_i,\bm x_i) \in \R \times \R^p$ is given by \citep[][Chap. 6.3]{hampel_ronchetti_etal_1986}
\begin{equation}\label{eq:gm_if}
    \mathrm{IF}(y_i, \bm x_i; \bm \theta, \sigma) = \frac{1}{N}\bm M\big(\bm \theta(F),\sigma \big)^{-1} \bm \Psi_i \big(y_i, \bm x_i;\bm \theta (F), \sigma \big) \qquad \text{for fixed} \; \sigma > 0,
\end{equation}
where $\bm M$ is the $(p \times p)$ matrix defined as
\begin{equation}\label{eq:gm_if_m}
    \bm M(\bm \theta, \sigma) = \sum_{i=1}^N \mathbbm{E}_{\xi} \big\{ \bm \Psi_i'(\bm \theta, \sigma) \big\}, \quad \text{where} \quad
    \bm \Psi_i'(\bm \theta, \sigma) = -\frac{\partial}{\partial \bm \theta^*} \bm \Psi_i(Y_i, \bm x_i; \bm \theta^*, \sigma) \bigg\vert_{\bm \theta^* = \bm \theta}.
\end{equation}

With sample data $\{(y_i, \bm x_i, v_i) : i \in s\}$, we cannot compute $\bm \theta_{N}$ as root of the estimating equation in (\ref{eq:gm_ee}). Therefore, $\bm \theta_{N}$ is said to define \citep[or induce,][]{godambe_1995} a parameter for the survey population. If $p(s)$ is a non-self-weighting sampling design, we consider estimating $\bm \theta_{N}$ as the root $\widehat{\bm \theta}_n$ of the sample-based \textit{optimal} estimating function \citep[in the sense of][]{godambe_thompson_1986} $\widehat{\bm \Psi}_n(\widehat{\bm \theta}_n, \sigma) = \bm 0$ (for fixed $\sigma > 0$), where
\begin{equation}\label{eq:gm_optimal}
    \widehat{\bm \Psi}_n(\bm \theta, \sigma) = \sum_{i \in s} \frac{\bm \Psi_i(\bm \theta, \sigma)}{\pi_i},
\end{equation}
\noindent or, more generally, the estimating functions
\begin{equation}\label{eq:gm_optimal}
    \widehat{\bm \Psi}_n(\bm \theta, \sigma) = \sum_{i \in s} w_i \bm \Psi_i(\bm \theta, \sigma),
\end{equation}
\noindent where the weights $w_i$ are calibrated such that $\mathbbm{E}_p \sum_{i \in s} w_i \bm z_i \approx \sum_{i \in U} \bm z_i$; the $\bm z_i \in \R^q$ are known calibration variables. The scale parameter $\sigma$ is unknown and must be estimated together with $\widehat{\bm \theta}_{n}$. The estimator of $\sigma$ is taken to be the (normalized) weighted median of the absolute deviations about the weighted median of the residuals (MAD), which is denoted by $\widehat{\sigma}$; see \citet{welsh_1986} for a theoretical justification of the MAD as estimator of the regression scale.

Estimating (super-) population parameters through their finite population counterparts entails some degree of robustness to deviations from the ideal conditions of the assumed model. The parameters defined this way are meaningful even if some of the model assumptions are violated. On the other hand, if we have strong beliefs in the validity of the (super-) population model (\ref{eq:reg_model}), there is no need for unbiased estimation of the finite population quantity $\bm \Psi_N$ by $\widehat{\bm \Psi}_n$. Instead, we would consider $\sum_{i \in s} \bm \Psi_i(\bm \theta, \sigma)$ as the estimating function of interest.

%-------------------------------------------------------------------------------
\section{Regression inference}

think of the difference between descriptive and analytic aims in terms of the difference in target populations, one being actual and definite, and the other being conceptual or hypothetical

type of attribute

``Descriptive inference is about some function of the values of a variate for unseen units of the survey population, and analytic inference is about the parameters of the model.'' \citep[][p. 200]{thompson_1997}.

The choice of

\citet{thompson_1997}

 

gives valid inference not only under the ideal conditions of the assumed model

\begin{itemize}
    \item model-based
    \item design-based
    \item compound model-design
\end{itemize}


even when some of the model assumptions are violated.

Let
\begin{equation}
    \bm \Psi_i'(\bm \theta, \sigma) = -\frac{\partial}{\partial \bm \theta^*} \bm \Psi_i(Y_i, \bm x_i; \bm \theta^*, \sigma) \bigg\vert_{\bm \theta^* = \bm \theta}.
\end{equation}


%-------------------------------------------------------------------------------
\subsection{Model-based covariance estimator}
Under suitable regularity conditions as $N \rightarrow \infty$, the regression $GM$-estimator of $\bm \theta_N$ has an asymptotic Gaussian distribution with mean $\bm \theta$ and asymptotic covariance matrix \citep[][Chap. 6.3]{hampel_ronchetti_etal_1986}
\begin{equation}\label{eq:reg_cov_model}
    \mathrm{cov}_{\xi}(\bm \theta_N, \sigma) = \bm M^{-1}(\bm \theta, \sigma) \cdot \bm Q(\bm \theta, \sigma) \cdot \bm M^{-T}(\bm \theta, \sigma) \qquad \text{for known} \; \sigma > 0,
\end{equation}
where $\bm M_N$ is defined in (\ref{eq:gm_if_m}) and $\bm Q(\bm \theta, \sigma) = (1/N) \sum \mathbbm{E}_{\xi} \{ \bm \Psi_i(Y_i, \bm x_i;\bm \theta, \sigma) \bm \Psi_i(Y_i, \bm x_i;\bm \theta, \sigma)^T \}$, where summation is over all $i=1,\ldots,N$. A sample-based estimator, $\widehat{\mathrm{cov}}_{\xi}(\bm \theta_N, \sigma)$, obtains by replacing the matrices $\bm M$ and $\bm Q$ in (\ref{eq:reg_cov_model}) with
\begin{equation*}
    \bm M_N(\bm \theta_N, \sigma) = \sum_{i=1}^N \bm \Psi_i'(\bm \theta_N, \sigma) \quad \text{and} \quad
    \bm Q_N(\bm \theta_N, \sigma) = \frac{1}{N} \sum_{i = 1}^N  \bm \Psi_i(\bm \theta_N, \sigma) \bm \Psi_i(\bm \theta_N, \sigma)^T.
\end{equation*}

%-------------------------------------------------------------------------------
\subsection{Design-based covariance estimator}
The design-based covariance estimator $\mathrm{cov}_p(\widehat{\bm \theta}_n)$ is derived from only the randomization $p$-distribution induced by the sampling design without reference to the (super-) population model in (\ref{eq:reg_model}). Using a first-order Taylor series linearization of $\widehat{\bm \Psi}_n(\widehat{\bm \theta}_n, \sigma)$ around $\widehat{\bm \theta}_n = \bm \theta_N$, we have \citep[see e.g.][]{binder_1983}
\begin{equation}\label{eq:reg_taylor}
    \widehat{\bm \theta}_n - \bm \theta_N = \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \widehat{\bm \Psi}_n(\bm \theta_N, \sigma) + \bm R \qquad \text{for all} \; \sigma > 0,
\end{equation}
\noindent where $\widehat{\bm M}_n(\bm \theta_N, \sigma) = \sum_{i \in s} \bm \Psi_i'(\bm \theta_N, \sigma) / \pi_i$. Under suitable regularity conditions as $(n, N) \rightarrow \infty$ \citep[see][]{binder_1983}, the remainder in (\ref{eq:reg_taylor}) vanishes and, asymptotically, the covariance matrix of $\widehat{\bm \theta_n}$ is

\begin{equation}\label{eq:reg_plim}
    \mathrm{cov}_{p} \big( \widehat{\bm \theta}_n \big) = \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \; \mathrm{cov}_p\Big\{\widehat{\bm \Psi}_n(\bm \theta_N, \sigma) \Big\} \; \widehat{\bm M}_n^{-T}(\bm \theta_N, \sigma) \qquad \text{for all} \; \sigma > 0.
\end{equation}
% \begin{align}\label{eq:reg_plim}
%     \mathrm{cov}_{p} \big( \widehat{\bm \theta}_n \big) &= \mathbbm{E}_p \Big( (\widehat{\bm \theta}_n - \bm \theta_N) (\widehat{\bm \theta}_n - \bm \theta_N)^T \Big) \nonumber \\
%     %
%     & = \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \; \mathrm{cov}_p\Big\{\widehat{\bm \Psi}_n(\bm \theta_N, \sigma) \Big\} \; \widehat{\bm M}_n^{-T}(\bm \theta_N, \sigma) \qquad \text{for all} \; \sigma > 0.
% \end{align}
Following \citet{binder_1983}, a plug-in type estimator of $\mathrm{cov}_p(\widehat{\bm \theta}_n)$ is
\begin{equation}
    \widehat{\mathrm{cov}}_p(\widehat{\bm \theta}_n) = \widehat{\bm M}_n^{-1}(\widehat{\bm \theta}_n, \widehat{\sigma}) \; \widehat{\mathrm{cov}}_p \Big\{ \widehat{\bm \Psi}_n(\widehat{\bm \theta}_n, \widehat{\sigma}) \Big\} \; \widehat{\bm M}_n^{-T}(\widehat{\bm \theta}_n, \widehat{\sigma}),
\end{equation}
\noindent where
\begin{equation*}
    \widehat{\mathrm{cov}}_p \big\{ \widehat{\bm \Psi}_n\big\} = \mathbbm{E}_p \Big\{ \big(\widehat{\bm \Psi}_n - \bm \Psi_N\big) \big(\widehat{\bm \Psi}_n - \bm \Psi_N\big)^T\Big\}.
\end{equation*}
\noindent is the estimated design-based covariance matrix of the vector-valued estimated total $\widehat{\bm \Psi}_n$.

%-------------------------------------------------------------------------------
\subsection{Compound model-design covariance estimator}

The design-model (asymptotic) $\xi p$-covariance matrix of the regression coefficient is
\begin{equation}\label{eq:reg_total_cov}
    \mathrm{cov}_{\xi p}\big( \widehat{\bm \theta}_n \big) = \mathbbm{E}_{\xi} \Big\{ \mathrm{cov}_{p} \big( \widehat{\bm \theta}_n \big) \Big\}+ \mathrm{cov}_{\xi} \Big\{ \mathbbm{E}_p \big( \widehat{\bm \theta}_n\big) \Big\}.
\end{equation}
\noindent Using a first-order Taylor series linearization of $\widehat{\bm \Psi}_n(\widehat{\bm \theta}_n, \sigma)$ around $\widehat{\bm \theta}_n = \bm \theta_N$, we have \citep[see e.g.][]{binder_1983}
\begin{equation}\label{eq:reg_taylor2}
    \widehat{\bm \theta}_n - \bm \theta_N = \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \widehat{\bm \Psi}_n(\bm \theta_N, \sigma) + \bm R \qquad \text{for all} \; \sigma > 0,
\end{equation}
\noindent where $\bm R$ is a remainder term and
\begin{equation}
    \widehat{\bm M}_n(\bm \theta_N, \sigma) = \frac{1}{N}\sum_{i \in s} \frac{1}{\pi_i}\mathbbm{E}_{\xi} \left\{ -\frac{\partial}{\partial \bm \theta} \bm \Psi_i(Y_i, \bm x_i; \bm \theta, \sigma) \bigg\vert_{\bm \theta = \bm \theta_N}\right\}.
\end{equation}
\noindent Under suitable regularity conditions as $(n, N) \rightarrow \infty$, the remainder in (\ref{eq:reg_taylor2}) vanishes $\bm R \rightarrow_p \bm 0$ (convergence in probability) and, asymptotically, we have
\begin{align}\label{eq:reg_plim}
    \mathrm{cov}_{p} \big( \widehat{\bm \theta}_n \big) &= \mathbbm{E}_p \Big( (\widehat{\bm \theta}_n - \bm \theta_N) (\widehat{\bm \theta}_n - \bm \theta_N)^T \Big) \nonumber \\
    %
    & = \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \; \mathrm{cov}_p\Big\{\widehat{\bm \Psi}_n(\bm \theta_N, \sigma) \Big\} \; \widehat{\bm M}_n^{-T}(\bm \theta_N, \sigma) \qquad \text{for all} \; \sigma > 0.
\end{align}
\noindent Hence, for large samples, (\ref{eq:reg_taylor2}) and $\mathbbm{E}_p \widehat{\bm \Psi}_n(\bm \theta_N, \sigma) = \bm 0$ imply that $\widehat{\bm \theta}_n$ is an asymptotically design unbiased estimator for $\bm \theta_N$, thus from (\ref{eq:reg_plim})
\begin{align*}
    \mathrm{cov}_{\xi p}\big( \widehat{\bm \theta}_n \big) & \rightarrow_p \mathbbm{E}_{\xi} \Big\{ \mathrm{cov}_{p} \big( \widehat{\bm \theta}_n\big) \Big\} + \mathrm{cov}_{\xi} \Big\{ \mathbbm{E}_p \big( \widehat{\bm \theta}_n\big) \Big\} \\
    %
    & =  \widehat{\bm M}_n^{-1}(\bm \theta_N, \sigma) \; \widehat{\bm Q}_n(\bm \theta_N, \sigma) \; \widehat{\bm M}_n^{-T}(\bm \theta_N, \sigma) +\mathrm{cov}_{\xi} \big( \bm \theta_N \big) \qquad \text{for all} \; \sigma > 0.
\end{align*}
\noindent where
\begin{equation*}
    \widehat{\bm Q}_n(\bm \theta_N, \sigma) = \sum_{i \in s} \frac{1}{\pi_i} \mathbbm{E}_{\xi} \Big\{ \bm \Psi_i(Y_i, \bm x_i;\bm \theta_N, \sigma) \bm \Psi_i(Y_i, \bm x_i;\bm \theta_N, \sigma)^T \Big\}.
\end{equation*}
\noindent Since $\bm \theta_N \rightarrow \bm \theta$ as $N \rightarrow \infty$, it follows that
\begin{align}\label{eq:reg_cov_xi_p}
    \mathrm{cov}_{\xi p}\big( \widehat{\bm \theta}_n \big) &\rightarrow_p \widehat{\bm M}_n^{-1}(\bm \theta, \sigma) \; \widehat{\bm Q}_n(\bm \theta, \sigma) \; \widehat{\bm M}_n^{-T}(\bm \theta, \sigma) +\mathrm{cov}_{\xi} \big( \bm \theta_N\big) \qquad \text{for all} \; \sigma > 0 \nonumber \\
    & = \bm V_1 + \bm V_2, \;\text{say}.
\end{align}
\noindent For many sampling designs, the term $\bm V_2$ in (\ref{eq:reg_cov_xi_p}) is $\mathcal{O}(N^{-1})$ and thus negligible compared with $\bm V_1$, which is $\mathcal{O}(n^{-1})$, if the the sampling fraction is small \citep{binder_roberts_2009}. Under these assumptions, the (asymptotic) covariance matrix can be estimated by the plug-in estimator
\begin{equation}\label{eq:reg_cov_est}
    \widehat{\mathrm{cov}}_{\xi p}\big( \widehat{\bm \theta}_n \big) = \widehat{\bm M}_n^{-1}(\widehat{\bm \theta}_n, \sigma) \; \widehat{\bm Q}_n(\widehat{\bm \theta}_n, \sigma) \; \widehat{\bm M}_n^{-T}(\widehat{\bm \theta}_n, \sigma) \qquad \text{for all} \; \sigma > 0.
\end{equation}


More specifically, we have---depending on the type of estimator---in place of $\widehat{\bm M}_*$ and $\widehat{\bm Q}_*$ in the template (\ref{eq:reg_cov_est})
\begin{enumerate}
   \item{$M$-estimator:}
   \begin{equation*}
      \widehat{\bm M}_M = - \overline{\psi'} \cdot \bm X^T \bm W \bm X, \qquad \widehat{\bm Q}_M = \overline{\psi^2} \cdot \bm X^T \bm W \bm X;
   \end{equation*}
   %
   \item{Mallows $GM$-estimator:}
   \begin{equation*}
      \widehat{\bm M}_{GM}^{m} = - \overline{\psi'} \cdot \bm X^T \bm W \bm H \bm X, \qquad \widehat{\bm Q}_{GM}^{m} = \overline{\psi^2} \cdot \bm X^T \bm W \bm H^2 \bm X;
   \end{equation*}
   %
   \item{Schweppe $GM$-estimator:}
   \begin{equation*}
      \widehat{\bm M}_{GM}^{s} = - \bm X^T \bm W \bm S_1 \bm X, \qquad \widehat{\bm Q}_{GM}^{s} = \bm X^T \bm W \bm S_2 \bm X,
   \end{equation*}
\end{enumerate}
\noindent where
\begin{align*}
    \bm W &= \mathrm{diag}_{i=1,\ldots,n}\{w_i\}, \\
    %
    \bm H &= \mathrm{diag}_{i=1,\ldots,n}\{h(\bm x_i)\}, \\
    %
    \overline{\psi'} &= \frac{1}{\widehat{N}}\sum_{i \in s} w_i \psi' \left( \frac{r_i}{\widehat{\sigma} \sqrt{v_i}} \right), \\
    %
    \overline{\psi^2} &= \frac{1}{\widehat{N}}\sum_{i \in s} w_i \psi^2 \left( \frac{r_i}{\widehat{\sigma} \sqrt{v_i}} \right), \\
    %
    \bm S_1 &= \mathrm{diag}_{i=1,\ldots,n} \big\{ s_1^i \big\}, \quad \text{with} \quad s_1^i = \frac{1}{\widehat{N}}\sum_{j \in s} w_j \psi'\left(\frac{r_j}{h(\bm x_i)\widehat{\sigma} \sqrt{v_j}}\right), \\
    %
    \bm S_2 &= \mathrm{diag}_{i=1,\ldots,n} \big\{ s_2^i \big\}, \quad \text{with} \quad s_2^i = \frac{1}{\widehat{N}}\sum_{j \in s} w_j \psi^2\left(\frac{r_j}{h(\bm x_i)\widehat{\sigma} \sqrt{v_j}}\right)
\end{align*}
\noindent and $r_i = y_i - \bm x_i^T \widehat{\bm \theta}_*$, where $\widehat{\bm \theta}_*$ is a placeholder that stands for one of the estimators $\widehat{\bm \theta}_M$, $\widehat{\bm \theta}_{GM}^m$ or $\widehat{\bm \theta}_{GM}^s$.

\begin{remarks}~
\vspace{-0.5em}
\begin{enumerate}[i)]
    \item  The $i$-th diagonal element of $\bm S_1$ and $\bm S_2$ depends on $h(\bm x_i)$, but the summation is over $j \in s$; see also \citet[][Chap. 6]{marazzi_1987}.
    \item When $\bm W$ is equal to the identity matrix $\bm I$, the asymptotic covariance of $\widehat{\bm \theta}_M$ is equal to the expression in \citet[][Eq. 6.5]{huber_1981}, which is implemented in the \code{R} packages \code{MASS} \citep[][see function \code{rlm}]{venables_ripley_2002} and \code{robeth} \citep[][see subroutine \code{KFFACV}]{marazzi_2020}.
   %
    \item For the Mallows and Schweppe type $GM$-estimators and given that $\bm W = \bm I$, the asymptotic covariance in (\ref{eq:reg_cov_est}) coincides with the one implemented in package/ library \code{robeth} for the option   \textit{averaged}\footnote{The \code{robeth} package/ library implements three variance approximation methods: a) expected, b) averaged, and c) empirical. Method a) reflects the willingness to put strong beliefs into the correctness of the model---a situation which is rare in finite population sampling. Method c) is of rather limited use.}; see \citet[][Chap. 4]{marazzi_1993} and \citet[][Chap. 2.6]{marazzi_1987} on the earlier \code{ROBETH-85} implementation.
\end{enumerate}
\end{remarks}






%-------------------------------------------------------------------------------
\subsection{Implementation}


Let $r_i(\bm \theta) = y_i - \bm x_i^T \bm \theta$; $w_i$ is the sampling weight; $h_i$ is the weight in the design space.

Let $\bm \theta^{t}$ (and $\sigma^{t}$) denote the trial value at the $t$-th iteration; and updated value, $\bm \theta^{t+1}$, is the root of the weighted LS estimating equation,
\begin{equation}
   \sum_{i \in s} \overline{\omega}_i \big( \bm \theta^t\big) r_i\big(\bm \theta^{t+1}\big) \bm x_i = \bm 0,
\end{equation}
\noindent where $\overline{\omega}_i$ is a placeholder for the Mallows and Schweppe weights, respectively, $\overline{\omega}_i^M$ and $\overline{\omega}_i^S$ being defined as
\begin{equation}
   \overline{\omega}_i^M = \frac{w_i h_i u_i^M}{v_i} \qquad \text{and} \qquad  \overline{\omega}_i^S = \frac{w_i u_i^S}{v_i}
\end{equation}
\noindent where (supressing the dependency of $u_i^M$, $u_i^S$, and $z_i$ on $\bm \theta$ and $\sigma$)
\begin{equation}
   u_i^M = \frac{\psi(z_i)}{z_i}, \qquad  u_i^S = \frac{\psi(z_i/h_i)}{z_i/h_i}, \quad \text{and} \quad z_i = \frac{r_i(\bm \theta)}{\sigma \sqrt{v_i}}.
\end{equation}



%------------------------------------------------------------------------------
\subsubsection{Implementation}
To fix notation, put $\bm w =\mathrm{diag}(\bm W)$, $\bm h = \mathrm{diag}(\bm H)$, denote the Hadamard product of the matrices $\bm A$ and $\bm B$ by $\bm A\circ \bm B$ and suppose that $\sqrt{\cdot}$ is applied element by element.

For $M$-estimators, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_m}
   (\bm X^T \bm W \bm X)^{-1}
\end{equation}
\noindent and is computed by the following method:
\begin{enumerate}[i)]
   \item Compute the factorization $\sqrt{\bm w} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Compute $\bm R^{-1} \bm R^{-T}$, which is equal to (\ref{eq:cov_m}); taking advantage of the triangular shape of $\bm R^{-1}$ and $\bm R^{-T}$ (LAPACK: \code{dtrmm}).
\end{enumerate}

\noindent For the Mallows type $GM$-estimator, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_mallows}
   (\bm X^T \bm W \bm H \bm X)^{-1} \bm X^T \bm W \bm H^2 \bm X (\bm X^T \bm W \bm H \bm X)^{-1}
\end{equation}
\noindent and is computed by the following method:
\begin{enumerate}[i)]
   \item Compute the QR factorization: $\sqrt{\bm w \cdot \bm h} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Define a new matrix: $\bm A \leftarrow \sqrt{\bm h} \circ \bm Q$ (extraction of $\bm Q$ matrix with LAPACK: \code{dorgqr}).
   %
   \item Update the matrix: $\bm A \leftarrow \bm A \bm R^{-T}$ (taking advantage of the triangular shape of $\bm R^{-1}$; LAPACK: \code{dtrmm}).
   %
   \item Compute $\bm A \bm A^T$, which corresponds to the expression in (\ref{eq:cov_mallows}); (LAPACK: \code{dgemm}).
\end{enumerate}

\noindent For the Schweppe type $GM$-estimator, the covariance matrix is (up to a scalar) equal to
\begin{equation}\label{eq:cov_schweppe}
   (\bm X^T \bm W \bm S_1 \bm X)^{-1} \bm X^T \bm W \bm S_2 \bm X (\bm X^T \bm W \bm S_1 \bm X)^{-1}.
\end{equation}
\noindent Put $\bm s_1 = \mathrm{diag}(\bm S_1)$, $\bm s_2 = \mathrm{diag}(\bm S_2)$, and let $\cdot / \cdot $ denote elemental division (i.e., the inverse of the Hadamard product). We use the following approach:
\begin{enumerate}[i)]
   \item Compute the factorization $\sqrt{\bm w \circ \bm s_1} \circ \bm X := \bm Q \bm R$ (LAPACK: \code{dgeqrf}).
   %
   \item Invert the upper triangular matrix $\bm R$ by backward substitution to get $\bm R^{-1}$ (LAPACK: \code{dtrtri}).
   %
   \item Define a new matrix: $\bm A \leftarrow \sqrt{\bm s_2 / \bm s_1 } \circ \bm Q$ (extraction of $\bm Q$ matrix with LAPACK: \code{dorgqr}).
   %
   \item Update the matrix: $\bm A \leftarrow \bm A \bm R^{-T}$ (taking advantage of the triangular shape of $\bm R^{-1}$; LAPACK: \code{dtrmm}).
   %
   \item Compute $\bm A \bm A^T$, which corresponds to the expression in (\ref{eq:cov_schweppe}); (LAPACK: \code{dgemm}).
\end{enumerate}


\begin{remark}
\citet{marazzi_1987} uses the Cholesky factorization (see subroutines \code{RTASKV} and \code{RTASKW}) which is computationally a bit cheaper than our QR factorization.
\end{remark}


%------------------------------------------------------------------------------
% BIB
{
\singlespacing
\bibliographystyle{ecta}
\bibliography{master}
}


%------------------------------------------------------------------------------
\appendix
\section{Appendix}

 %------------------------------------------------------------------------------
\section{Robust GREG estimator}\label{ch:greg}
Consider the (super-) population model
\begin{equation*}
   \xi: \quad Y_i = \bm x_i^T\bm \beta + E_i \qquad(i=1,\ldots, n),
\end{equation*}
\noindent where 
\begin{enumerate}[i)]
   \item $\bm \beta \in \R^p$ ($p < n$),
   %
   \item $\mathbb{E}_{\xi}(E_i)=0$,  
   %
   \item $var_{\xi}(E_i) = c_i \sigma^2$ with $\sigma > 0$ and the $c_i$'s are (known) constants such that $c_i > 0$,
   %
   \item $\mathbb{E}_{\xi}(E_iE_j)=0$ for all $i \neq j$. 
\end{enumerate}

\noindent The parameters $\bm \beta$ and $\sigma^2$ are subject to estimation. Put $\bm X = (\bm x_1^T, \ldots, \bm x_n^T)^T$, where $\bm x_i = (x_{i,1}, \ldots, x_{i,p})^T$. In addition, let $\bm y = (y_1, \ldots, y_n)^T$, $y_i$ denoting the realization of $Y_i$. 

Model $\xi$ can 
\begin{enumerate}[i)]
   \item include a regression intercept (then, we have $x_{i,1} \equiv 1$)  or it can 
   %
   \item be specified as a regression through the origin (RTO).  
\end{enumerate}

Let $\bm b^{(0)}$ denote the weighted least squares (LS) estimate of $\bm \beta$; it serves as our initial estimate. Let  $s^{(0)}$ denote the weighted median of the absolute deviations of the residuals about the weighted median of the residuals (weighted MAD). When $s^{(0)}$ is zero, we compute the weighted interquartile range instead. In contrast to method \texttt{rlm} in package \texttt{MASS}, we do not use the weighted median of the residuals \textit{about zero} as weighted MAD because this scale estimate is not appropriate for the RTO model.\footnote{Under the RTO model, the mean of the residuals is not necessarily equal to zero.} 

Define the function $u:\R \rightarrow \R_+$ given by 
\begin{equation*}
   u(x) = \frac{\psi_k(x)}{x},
\end{equation*}
\noindent where $\psi_k$ denotes the Huber $\psi$-function, and let the residuals be (for any $\bm b \in \R$ and all $i=1,\ldots,n$) 
\begin{equation*}
   r_i(\bm b) = y_i - \bm x_i^T \bm b.
\end{equation*}

Starting with $(\bm \beta^{(0)}, s^{(0)})$, we compute iteratively refined estimates $(\bm \beta^{(\tau)}, s^{(\tau)})$, where $\tau=1,2,\ldots$ counts the number of iterations, 
\begin{align*}
   \bm \beta^{(\tau + 1)} &\leftarrow \text{weighted least squares}\left\{ \mathrm{weights} = w_i\cdot u\left( \frac{r_i\big(\bm \beta^{(\tau)}\big)}{s^{(\tau)}}\right), \bm X, \bm y \right\} \\ 
   %
   s^{(\tau + 1)} &\leftarrow \text{weighted mad}\left\{ \frac{r_i\big(\bm \beta^{(\tau)}\big)}{s^{(\tau)}},\; \mathrm{weights}=w_i \right\} 
\end{align*}
\noindent until the termination rule,
\begin{equation*}
   \big\Vert \bm \beta^{(\tau + 1)} - \bm \beta^{(\tau)} \big\Vert \leq \mathrm{tol} \cdot s^{(\tau + 1)},
\end{equation*}
\noindent is satisfied for some predetermined value $\mathrm{tol}$ (e.g., $\mathrm{tol} = 10^{-5}$). The final result of the iterative updating scheme is denoted by $\widehat{\bm \beta}$ and $\widehat{s}$. 

The joint estimator $(\widehat{\bm \beta}, \, \widehat{s})$ features all the well known properties of an $M$-regression estimator. In particular, $\widehat{s}$ is an affine invariant and scale equivariant estimator of the (super-) population MAD $s$ provied that $\bm \beta^{(0)}$ is a $\sqrt{n}$-consistent, affine and scale equivariant estimator of $\bm \beta$ (this is indeed the case since we use the LS estimator to obtain $\bm \beta^{(0)}$. Under further regularity conditions on the c.d.f. $F$ of the model errors $E_i$, \citet{welsh_1986} shows that ---under the assumption that all design weights are unity---$\widehat{s}$ is a $\sqrt{n}$ consistent estimator of $s$. We conjecture that this property generalizes to the case of non-unity sampling weights. Moreover, $\widehat{\bm \beta}$ has asymptotic linear representation in terms of the influence (from which we can easily derive a CLT); see \citet{welsh_1986} or \citet[][ch. 5]{jureckova_sen_1996}.

Before we continue our discussion, we shall address an important property of the joint estimate $(\widehat{\bm \beta}, \, \widehat{s})$. That is, if we let $k \rightarrow \infty$ then the robustness weights are unity for all $n$ observations; hence, estimate $\widehat{\bm \beta}$ is the weighted LS estimate and $\widehat{s}$ is the (normalized) weighted MAD of the LS residuals. This implies that statistical inference for $\bm \beta$ is different ---even if we account for the residual degrees of freedom--- from (classcical) inference statistics for the weighted LS estimate because it is based  on the weighted MAD $\widehat{s}$ instead the estimated residual standard deviation. In order to prevent such an unpleasant situtation, we shall work with a regression estimate of scale other than $\widehat{s}$. To this end, we introduce a close relative of the Huber 'Proposal 2' estimator of scale. This estimator has the nice property that it converges smoothly to the LS regression estimate of scale as $k \rightarrow \infty$.  

We define the robustness weights ($i=1, \ldots, n$)
\begin{equation}\label{eq:regressionscale1}
   u_i = u\left(\frac{r_i(\widehat{\bm \beta})}{\widehat{s}}\right)
\end{equation}
\noindent and compute the following regression estimate of scale
\begin{equation}\label{eq:regressionscale2}
   \widehat{\sigma} = \left[ \frac{1}{\kappa \widehat{N}}\sum_{i=1}^n w_i u_i^2 r_i^2\big(\widehat{\bm \beta}\big) \right]^{1/2}, 
\end{equation}
\noindent where 
\begin{equation*}
   \widehat{N} = \sum_{i=1}^n w_i,
\end{equation*}
\noindent and $\kappa$ is a correction term which ensures that $\widehat{\sigma}$ is a Fisher consistent estimator of $\sigma$ at the Gaussian core model; i.e., $\kappa = \mathbb{E}\big[\psi_k^2(U)\big]$ where $U \sim N(0,1)$. Note that we define the robustness weights in (\ref{eq:regressionscale1}) in terms of the (weighted) MAD-based estimate of scale, $\widehat{s}$, and the estimated residuals. The estimate of scale $\widehat{\sigma}$ then derives from this. The estimate defined in (\ref{eq:regressionscale2}) does not account for the incurred loss of degrees of freedom; this will be treated later. 



\end{document}
